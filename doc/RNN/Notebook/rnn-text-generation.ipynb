{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자연어 생성 모델\n",
    "---\n",
    "https://wikidocs.net/45101 참조한 예제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"경마장에 있는 말이 뛰고 있다\\n\n",
    "그의 말이 법이다\\n\n",
    "가는 말이 고와야 오는 말이 곱다\\n\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 12\n"
     ]
    }
   ],
   "source": [
    "# corpus 단어 토큰화\n",
    "t = Tokenizer()\n",
    "\n",
    "#전체 corpus 빈도기반 tokenizing 수행 -> 빈도가 높을수록 높은 번호를 부여\n",
    "t.fit_on_texts([text])\n",
    "\n",
    "# 케라스 토크나이저의 정수 인코딩은 인덱스가 1부터 시작하지만,\n",
    "# 케라스 원-핫 인코딩에서 배열의 인덱스가 0부터 시작하기 때문에\n",
    "# 배열의 크기를 실제 단어 집합의 크기보다 +1로 생성해야하므로 미리 +1 선언 \n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "print('단어 집합의 크기 : %d' % vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'말이': 1,\n",
       " '경마장에': 2,\n",
       " '있는': 3,\n",
       " '뛰고': 4,\n",
       " '있다': 5,\n",
       " '그의': 6,\n",
       " '법이다': 7,\n",
       " '가는': 8,\n",
       " '고와야': 9,\n",
       " '오는': 10,\n",
       " '곱다': 11}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 1, 4, 5]\n",
      "[]\n",
      "[6, 1, 7]\n",
      "[]\n",
      "[8, 1, 9, 10, 1, 11]\n",
      "[]\n",
      "학습에 사용할 샘플의 개수: 11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[2, 3],\n",
       " [2, 3, 1],\n",
       " [2, 3, 1, 4],\n",
       " [2, 3, 1, 4, 5],\n",
       " [6, 1],\n",
       " [6, 1, 7],\n",
       " [8, 1],\n",
       " [8, 1, 9],\n",
       " [8, 1, 9, 10],\n",
       " [8, 1, 9, 10, 1],\n",
       " [8, 1, 9, 10, 1, 11]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습데이터 생성\n",
    "\n",
    "sequences = list()\n",
    "for line in text.split('\\n'): # Wn을 기준으로 문장 토큰화\n",
    "    encoded = t.texts_to_sequences([line])[0] #원래 문장을 토큰화된 수치로 치환\n",
    "    print(encoded)\n",
    "    for i in range(1, len(encoded)): #원래 문장을 찢어 학습데이터로 생성\n",
    "        sequence = encoded[:i+1]\n",
    "        sequences.append(sequence)\n",
    "\n",
    "print('학습에 사용할 샘플의 개수: %d' % len(sequences))\n",
    "\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플의 최대 길이 : 6\n"
     ]
    }
   ],
   "source": [
    "# 학습데이터 길이 일치화\n",
    "# 길이가 일치해야 RNN에 넣을 수 있다\n",
    "max_len=max(len(l) for l in sequences) # 모든 샘플에서 길이가 가장 긴 샘플의 길이 출력\n",
    "print('샘플의 최대 길이 : {}'.format(max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  2,  3],\n",
       "       [ 0,  0,  0,  2,  3,  1],\n",
       "       [ 0,  0,  2,  3,  1,  4],\n",
       "       [ 0,  2,  3,  1,  4,  5],\n",
       "       [ 0,  0,  0,  0,  6,  1],\n",
       "       [ 0,  0,  0,  6,  1,  7],\n",
       "       [ 0,  0,  0,  0,  8,  1],\n",
       "       [ 0,  0,  0,  8,  1,  9],\n",
       "       [ 0,  0,  8,  1,  9, 10],\n",
       "       [ 0,  8,  1,  9, 10,  1],\n",
       "       [ 8,  1,  9, 10,  1, 11]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0,  0,  0,  0,  2],\n",
       "        [ 0,  0,  0,  2,  3],\n",
       "        [ 0,  0,  2,  3,  1],\n",
       "        [ 0,  2,  3,  1,  4],\n",
       "        [ 0,  0,  0,  0,  6],\n",
       "        [ 0,  0,  0,  6,  1],\n",
       "        [ 0,  0,  0,  0,  8],\n",
       "        [ 0,  0,  0,  8,  1],\n",
       "        [ 0,  0,  8,  1,  9],\n",
       "        [ 0,  8,  1,  9, 10],\n",
       "        [ 8,  1,  9, 10,  1]]),\n",
       " array([ 3,  1,  4,  5,  1,  7,  1,  9, 10,  1, 11]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data/label 분리\n",
    "# 문장의 마지막 단어를 label로 설정\n",
    "sequences = np.array(sequences)\n",
    "X = sequences[:,:-1]\n",
    "y = sequences[:,-1]\n",
    "\n",
    "X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label One-Hot Encoding \n",
    "# 딥러닝 네트워크는 오직 벡터 형태로만 출력을 해준다\n",
    "# 단일값으로만 된 라벨을 one-hot encoding 해줘야 딥러닝 출력과 비교 가능\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11 samples\n",
      "Epoch 1/200\n",
      "11/11 - 0s - loss: 2.4614 - acc: 0.3636\n",
      "Epoch 2/200\n",
      "11/11 - 0s - loss: 2.4486 - acc: 0.3636\n",
      "Epoch 3/200\n",
      "11/11 - 0s - loss: 2.4354 - acc: 0.4545\n",
      "Epoch 4/200\n",
      "11/11 - 0s - loss: 2.4219 - acc: 0.4545\n",
      "Epoch 5/200\n",
      "11/11 - 0s - loss: 2.4080 - acc: 0.4545\n",
      "Epoch 6/200\n",
      "11/11 - 0s - loss: 2.3936 - acc: 0.4545\n",
      "Epoch 7/200\n",
      "11/11 - 0s - loss: 2.3787 - acc: 0.4545\n",
      "Epoch 8/200\n",
      "11/11 - 0s - loss: 2.3632 - acc: 0.4545\n",
      "Epoch 9/200\n",
      "11/11 - 0s - loss: 2.3471 - acc: 0.4545\n",
      "Epoch 10/200\n",
      "11/11 - 0s - loss: 2.3303 - acc: 0.4545\n",
      "Epoch 11/200\n",
      "11/11 - 0s - loss: 2.3128 - acc: 0.4545\n",
      "Epoch 12/200\n",
      "11/11 - 0s - loss: 2.2945 - acc: 0.3636\n",
      "Epoch 13/200\n",
      "11/11 - 0s - loss: 2.2754 - acc: 0.3636\n",
      "Epoch 14/200\n",
      "11/11 - 0s - loss: 2.2555 - acc: 0.3636\n",
      "Epoch 15/200\n",
      "11/11 - 0s - loss: 2.2348 - acc: 0.3636\n",
      "Epoch 16/200\n",
      "11/11 - 0s - loss: 2.2132 - acc: 0.3636\n",
      "Epoch 17/200\n",
      "11/11 - 0s - loss: 2.1909 - acc: 0.3636\n",
      "Epoch 18/200\n",
      "11/11 - 0s - loss: 2.1678 - acc: 0.3636\n",
      "Epoch 19/200\n",
      "11/11 - 0s - loss: 2.1441 - acc: 0.3636\n",
      "Epoch 20/200\n",
      "11/11 - 0s - loss: 2.1198 - acc: 0.3636\n",
      "Epoch 21/200\n",
      "11/11 - 0s - loss: 2.0952 - acc: 0.3636\n",
      "Epoch 22/200\n",
      "11/11 - 0s - loss: 2.0704 - acc: 0.3636\n",
      "Epoch 23/200\n",
      "11/11 - 0s - loss: 2.0456 - acc: 0.3636\n",
      "Epoch 24/200\n",
      "11/11 - 0s - loss: 2.0213 - acc: 0.3636\n",
      "Epoch 25/200\n",
      "11/11 - 0s - loss: 1.9975 - acc: 0.3636\n",
      "Epoch 26/200\n",
      "11/11 - 0s - loss: 1.9746 - acc: 0.3636\n",
      "Epoch 27/200\n",
      "11/11 - 0s - loss: 1.9529 - acc: 0.3636\n",
      "Epoch 28/200\n",
      "11/11 - 0s - loss: 1.9324 - acc: 0.3636\n",
      "Epoch 29/200\n",
      "11/11 - 0s - loss: 1.9134 - acc: 0.3636\n",
      "Epoch 30/200\n",
      "11/11 - 0s - loss: 1.8955 - acc: 0.3636\n",
      "Epoch 31/200\n",
      "11/11 - 0s - loss: 1.8788 - acc: 0.3636\n",
      "Epoch 32/200\n",
      "11/11 - 0s - loss: 1.8628 - acc: 0.3636\n",
      "Epoch 33/200\n",
      "11/11 - 0s - loss: 1.8471 - acc: 0.3636\n",
      "Epoch 34/200\n",
      "11/11 - 0s - loss: 1.8314 - acc: 0.3636\n",
      "Epoch 35/200\n",
      "11/11 - 0s - loss: 1.8154 - acc: 0.3636\n",
      "Epoch 36/200\n",
      "11/11 - 0s - loss: 1.7987 - acc: 0.3636\n",
      "Epoch 37/200\n",
      "11/11 - 0s - loss: 1.7813 - acc: 0.3636\n",
      "Epoch 38/200\n",
      "11/11 - 0s - loss: 1.7632 - acc: 0.3636\n",
      "Epoch 39/200\n",
      "11/11 - 0s - loss: 1.7444 - acc: 0.3636\n",
      "Epoch 40/200\n",
      "11/11 - 0s - loss: 1.7251 - acc: 0.3636\n",
      "Epoch 41/200\n",
      "11/11 - 0s - loss: 1.7053 - acc: 0.3636\n",
      "Epoch 42/200\n",
      "11/11 - 0s - loss: 1.6852 - acc: 0.3636\n",
      "Epoch 43/200\n",
      "11/11 - 0s - loss: 1.6650 - acc: 0.3636\n",
      "Epoch 44/200\n",
      "11/11 - 0s - loss: 1.6445 - acc: 0.3636\n",
      "Epoch 45/200\n",
      "11/11 - 0s - loss: 1.6240 - acc: 0.3636\n",
      "Epoch 46/200\n",
      "11/11 - 0s - loss: 1.6034 - acc: 0.3636\n",
      "Epoch 47/200\n",
      "11/11 - 0s - loss: 1.5826 - acc: 0.4545\n",
      "Epoch 48/200\n",
      "11/11 - 0s - loss: 1.5616 - acc: 0.5455\n",
      "Epoch 49/200\n",
      "11/11 - 0s - loss: 1.5404 - acc: 0.5455\n",
      "Epoch 50/200\n",
      "11/11 - 0s - loss: 1.5190 - acc: 0.5455\n",
      "Epoch 51/200\n",
      "11/11 - 0s - loss: 1.4973 - acc: 0.6364\n",
      "Epoch 52/200\n",
      "11/11 - 0s - loss: 1.4753 - acc: 0.6364\n",
      "Epoch 53/200\n",
      "11/11 - 0s - loss: 1.4531 - acc: 0.6364\n",
      "Epoch 54/200\n",
      "11/11 - 0s - loss: 1.4307 - acc: 0.6364\n",
      "Epoch 55/200\n",
      "11/11 - 0s - loss: 1.4081 - acc: 0.6364\n",
      "Epoch 56/200\n",
      "11/11 - 0s - loss: 1.3854 - acc: 0.6364\n",
      "Epoch 57/200\n",
      "11/11 - 0s - loss: 1.3626 - acc: 0.6364\n",
      "Epoch 58/200\n",
      "11/11 - 0s - loss: 1.3398 - acc: 0.6364\n",
      "Epoch 59/200\n",
      "11/11 - 0s - loss: 1.3170 - acc: 0.6364\n",
      "Epoch 60/200\n",
      "11/11 - 0s - loss: 1.2943 - acc: 0.6364\n",
      "Epoch 61/200\n",
      "11/11 - 0s - loss: 1.2718 - acc: 0.6364\n",
      "Epoch 62/200\n",
      "11/11 - 0s - loss: 1.2494 - acc: 0.6364\n",
      "Epoch 63/200\n",
      "11/11 - 0s - loss: 1.2272 - acc: 0.7273\n",
      "Epoch 64/200\n",
      "11/11 - 0s - loss: 1.2053 - acc: 0.7273\n",
      "Epoch 65/200\n",
      "11/11 - 0s - loss: 1.1836 - acc: 0.7273\n",
      "Epoch 66/200\n",
      "11/11 - 0s - loss: 1.1622 - acc: 0.7273\n",
      "Epoch 67/200\n",
      "11/11 - 0s - loss: 1.1412 - acc: 0.7273\n",
      "Epoch 68/200\n",
      "11/11 - 0s - loss: 1.1204 - acc: 0.7273\n",
      "Epoch 69/200\n",
      "11/11 - 0s - loss: 1.1000 - acc: 0.7273\n",
      "Epoch 70/200\n",
      "11/11 - 0s - loss: 1.0799 - acc: 0.7273\n",
      "Epoch 71/200\n",
      "11/11 - 0s - loss: 1.0601 - acc: 0.7273\n",
      "Epoch 72/200\n",
      "11/11 - 0s - loss: 1.0406 - acc: 0.7273\n",
      "Epoch 73/200\n",
      "11/11 - 0s - loss: 1.0215 - acc: 0.7273\n",
      "Epoch 74/200\n",
      "11/11 - 0s - loss: 1.0027 - acc: 0.7273\n",
      "Epoch 75/200\n",
      "11/11 - 0s - loss: 0.9843 - acc: 0.7273\n",
      "Epoch 76/200\n",
      "11/11 - 0s - loss: 0.9662 - acc: 0.7273\n",
      "Epoch 77/200\n",
      "11/11 - 0s - loss: 0.9484 - acc: 0.7273\n",
      "Epoch 78/200\n",
      "11/11 - 0s - loss: 0.9309 - acc: 0.7273\n",
      "Epoch 79/200\n",
      "11/11 - 0s - loss: 0.9137 - acc: 0.7273\n",
      "Epoch 80/200\n",
      "11/11 - 0s - loss: 0.8969 - acc: 0.7273\n",
      "Epoch 81/200\n",
      "11/11 - 0s - loss: 0.8804 - acc: 0.7273\n",
      "Epoch 82/200\n",
      "11/11 - 0s - loss: 0.8642 - acc: 0.7273\n",
      "Epoch 83/200\n",
      "11/11 - 0s - loss: 0.8483 - acc: 0.7273\n",
      "Epoch 84/200\n",
      "11/11 - 0s - loss: 0.8327 - acc: 0.7273\n",
      "Epoch 85/200\n",
      "11/11 - 0s - loss: 0.8175 - acc: 0.7273\n",
      "Epoch 86/200\n",
      "11/11 - 0s - loss: 0.8025 - acc: 0.7273\n",
      "Epoch 87/200\n",
      "11/11 - 0s - loss: 0.7878 - acc: 0.7273\n",
      "Epoch 88/200\n",
      "11/11 - 0s - loss: 0.7734 - acc: 0.8182\n",
      "Epoch 89/200\n",
      "11/11 - 0s - loss: 0.7594 - acc: 0.8182\n",
      "Epoch 90/200\n",
      "11/11 - 0s - loss: 0.7455 - acc: 0.8182\n",
      "Epoch 91/200\n",
      "11/11 - 0s - loss: 0.7320 - acc: 0.8182\n",
      "Epoch 92/200\n",
      "11/11 - 0s - loss: 0.7188 - acc: 0.8182\n",
      "Epoch 93/200\n",
      "11/11 - 0s - loss: 0.7058 - acc: 0.8182\n",
      "Epoch 94/200\n",
      "11/11 - 0s - loss: 0.6931 - acc: 0.8182\n",
      "Epoch 95/200\n",
      "11/11 - 0s - loss: 0.6806 - acc: 0.8182\n",
      "Epoch 96/200\n",
      "11/11 - 0s - loss: 0.6684 - acc: 0.8182\n",
      "Epoch 97/200\n",
      "11/11 - 0s - loss: 0.6565 - acc: 0.8182\n",
      "Epoch 98/200\n",
      "11/11 - 0s - loss: 0.6447 - acc: 0.8182\n",
      "Epoch 99/200\n",
      "11/11 - 0s - loss: 0.6333 - acc: 0.8182\n",
      "Epoch 100/200\n",
      "11/11 - 0s - loss: 0.6220 - acc: 0.8182\n",
      "Epoch 101/200\n",
      "11/11 - 0s - loss: 0.6110 - acc: 0.8182\n",
      "Epoch 102/200\n",
      "11/11 - 0s - loss: 0.6003 - acc: 0.8182\n",
      "Epoch 103/200\n",
      "11/11 - 0s - loss: 0.5897 - acc: 0.8182\n",
      "Epoch 104/200\n",
      "11/11 - 0s - loss: 0.5794 - acc: 0.8182\n",
      "Epoch 105/200\n",
      "11/11 - 0s - loss: 0.5692 - acc: 0.8182\n",
      "Epoch 106/200\n",
      "11/11 - 0s - loss: 0.5593 - acc: 0.9091\n",
      "Epoch 107/200\n",
      "11/11 - 0s - loss: 0.5496 - acc: 0.9091\n",
      "Epoch 108/200\n",
      "11/11 - 0s - loss: 0.5401 - acc: 0.9091\n",
      "Epoch 109/200\n",
      "11/11 - 0s - loss: 0.5307 - acc: 0.9091\n",
      "Epoch 110/200\n",
      "11/11 - 0s - loss: 0.5216 - acc: 0.9091\n",
      "Epoch 111/200\n",
      "11/11 - 0s - loss: 0.5126 - acc: 0.9091\n",
      "Epoch 112/200\n",
      "11/11 - 0s - loss: 0.5038 - acc: 0.9091\n",
      "Epoch 113/200\n",
      "11/11 - 0s - loss: 0.4952 - acc: 0.9091\n",
      "Epoch 114/200\n",
      "11/11 - 0s - loss: 0.4868 - acc: 0.9091\n",
      "Epoch 115/200\n",
      "11/11 - 0s - loss: 0.4785 - acc: 0.9091\n",
      "Epoch 116/200\n",
      "11/11 - 0s - loss: 0.4704 - acc: 0.9091\n",
      "Epoch 117/200\n",
      "11/11 - 0s - loss: 0.4625 - acc: 0.9091\n",
      "Epoch 118/200\n",
      "11/11 - 0s - loss: 0.4547 - acc: 0.9091\n",
      "Epoch 119/200\n",
      "11/11 - 0s - loss: 0.4471 - acc: 0.9091\n",
      "Epoch 120/200\n",
      "11/11 - 0s - loss: 0.4396 - acc: 0.9091\n",
      "Epoch 121/200\n",
      "11/11 - 0s - loss: 0.4322 - acc: 0.9091\n",
      "Epoch 122/200\n",
      "11/11 - 0s - loss: 0.4250 - acc: 0.9091\n",
      "Epoch 123/200\n",
      "11/11 - 0s - loss: 0.4180 - acc: 0.9091\n",
      "Epoch 124/200\n",
      "11/11 - 0s - loss: 0.4110 - acc: 0.9091\n",
      "Epoch 125/200\n",
      "11/11 - 0s - loss: 0.4042 - acc: 0.9091\n",
      "Epoch 126/200\n",
      "11/11 - 0s - loss: 0.3975 - acc: 0.9091\n",
      "Epoch 127/200\n",
      "11/11 - 0s - loss: 0.3910 - acc: 0.9091\n",
      "Epoch 128/200\n",
      "11/11 - 0s - loss: 0.3846 - acc: 0.9091\n",
      "Epoch 129/200\n",
      "11/11 - 0s - loss: 0.3783 - acc: 0.9091\n",
      "Epoch 130/200\n",
      "11/11 - 0s - loss: 0.3721 - acc: 0.9091\n",
      "Epoch 131/200\n",
      "11/11 - 0s - loss: 0.3660 - acc: 0.9091\n",
      "Epoch 132/200\n",
      "11/11 - 0s - loss: 0.3600 - acc: 0.9091\n",
      "Epoch 133/200\n",
      "11/11 - 0s - loss: 0.3542 - acc: 0.9091\n",
      "Epoch 134/200\n",
      "11/11 - 0s - loss: 0.3484 - acc: 0.9091\n",
      "Epoch 135/200\n",
      "11/11 - 0s - loss: 0.3427 - acc: 0.9091\n",
      "Epoch 136/200\n",
      "11/11 - 0s - loss: 0.3372 - acc: 0.9091\n",
      "Epoch 137/200\n",
      "11/11 - 0s - loss: 0.3317 - acc: 0.9091\n",
      "Epoch 138/200\n",
      "11/11 - 0s - loss: 0.3264 - acc: 0.9091\n",
      "Epoch 139/200\n",
      "11/11 - 0s - loss: 0.3211 - acc: 0.9091\n",
      "Epoch 140/200\n",
      "11/11 - 0s - loss: 0.3159 - acc: 0.9091\n",
      "Epoch 141/200\n",
      "11/11 - 0s - loss: 0.3108 - acc: 0.9091\n",
      "Epoch 142/200\n",
      "11/11 - 0s - loss: 0.3058 - acc: 0.9091\n",
      "Epoch 143/200\n",
      "11/11 - 0s - loss: 0.3009 - acc: 0.9091\n",
      "Epoch 144/200\n",
      "11/11 - 0s - loss: 0.2960 - acc: 0.9091\n",
      "Epoch 145/200\n",
      "11/11 - 0s - loss: 0.2913 - acc: 0.9091\n",
      "Epoch 146/200\n",
      "11/11 - 0s - loss: 0.2866 - acc: 0.9091\n",
      "Epoch 147/200\n",
      "11/11 - 0s - loss: 0.2820 - acc: 0.9091\n",
      "Epoch 148/200\n",
      "11/11 - 0s - loss: 0.2775 - acc: 0.9091\n",
      "Epoch 149/200\n",
      "11/11 - 0s - loss: 0.2730 - acc: 0.9091\n",
      "Epoch 150/200\n",
      "11/11 - 0s - loss: 0.2686 - acc: 0.9091\n",
      "Epoch 151/200\n",
      "11/11 - 0s - loss: 0.2643 - acc: 0.9091\n",
      "Epoch 152/200\n",
      "11/11 - 0s - loss: 0.2600 - acc: 0.9091\n",
      "Epoch 153/200\n",
      "11/11 - 0s - loss: 0.2559 - acc: 0.9091\n",
      "Epoch 154/200\n",
      "11/11 - 0s - loss: 0.2518 - acc: 0.9091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/200\n",
      "11/11 - 0s - loss: 0.2477 - acc: 1.0000\n",
      "Epoch 156/200\n",
      "11/11 - 0s - loss: 0.2437 - acc: 1.0000\n",
      "Epoch 157/200\n",
      "11/11 - 0s - loss: 0.2398 - acc: 1.0000\n",
      "Epoch 158/200\n",
      "11/11 - 0s - loss: 0.2360 - acc: 1.0000\n",
      "Epoch 159/200\n",
      "11/11 - 0s - loss: 0.2322 - acc: 1.0000\n",
      "Epoch 160/200\n",
      "11/11 - 0s - loss: 0.2285 - acc: 1.0000\n",
      "Epoch 161/200\n",
      "11/11 - 0s - loss: 0.2248 - acc: 1.0000\n",
      "Epoch 162/200\n",
      "11/11 - 0s - loss: 0.2212 - acc: 1.0000\n",
      "Epoch 163/200\n",
      "11/11 - 0s - loss: 0.2176 - acc: 1.0000\n",
      "Epoch 164/200\n",
      "11/11 - 0s - loss: 0.2141 - acc: 1.0000\n",
      "Epoch 165/200\n",
      "11/11 - 0s - loss: 0.2107 - acc: 1.0000\n",
      "Epoch 166/200\n",
      "11/11 - 0s - loss: 0.2073 - acc: 1.0000\n",
      "Epoch 167/200\n",
      "11/11 - 0s - loss: 0.2040 - acc: 1.0000\n",
      "Epoch 168/200\n",
      "11/11 - 0s - loss: 0.2007 - acc: 1.0000\n",
      "Epoch 169/200\n",
      "11/11 - 0s - loss: 0.1975 - acc: 1.0000\n",
      "Epoch 170/200\n",
      "11/11 - 0s - loss: 0.1943 - acc: 1.0000\n",
      "Epoch 171/200\n",
      "11/11 - 0s - loss: 0.1912 - acc: 1.0000\n",
      "Epoch 172/200\n",
      "11/11 - 0s - loss: 0.1881 - acc: 1.0000\n",
      "Epoch 173/200\n",
      "11/11 - 0s - loss: 0.1851 - acc: 1.0000\n",
      "Epoch 174/200\n",
      "11/11 - 0s - loss: 0.1821 - acc: 1.0000\n",
      "Epoch 175/200\n",
      "11/11 - 0s - loss: 0.1792 - acc: 1.0000\n",
      "Epoch 176/200\n",
      "11/11 - 0s - loss: 0.1764 - acc: 1.0000\n",
      "Epoch 177/200\n",
      "11/11 - 0s - loss: 0.1735 - acc: 1.0000\n",
      "Epoch 178/200\n",
      "11/11 - 0s - loss: 0.1708 - acc: 1.0000\n",
      "Epoch 179/200\n",
      "11/11 - 0s - loss: 0.1680 - acc: 1.0000\n",
      "Epoch 180/200\n",
      "11/11 - 0s - loss: 0.1653 - acc: 1.0000\n",
      "Epoch 181/200\n",
      "11/11 - 0s - loss: 0.1627 - acc: 1.0000\n",
      "Epoch 182/200\n",
      "11/11 - 0s - loss: 0.1601 - acc: 1.0000\n",
      "Epoch 183/200\n",
      "11/11 - 0s - loss: 0.1576 - acc: 1.0000\n",
      "Epoch 184/200\n",
      "11/11 - 0s - loss: 0.1551 - acc: 1.0000\n",
      "Epoch 185/200\n",
      "11/11 - 0s - loss: 0.1526 - acc: 1.0000\n",
      "Epoch 186/200\n",
      "11/11 - 0s - loss: 0.1502 - acc: 1.0000\n",
      "Epoch 187/200\n",
      "11/11 - 0s - loss: 0.1478 - acc: 1.0000\n",
      "Epoch 188/200\n",
      "11/11 - 0s - loss: 0.1455 - acc: 1.0000\n",
      "Epoch 189/200\n",
      "11/11 - 0s - loss: 0.1432 - acc: 1.0000\n",
      "Epoch 190/200\n",
      "11/11 - 0s - loss: 0.1409 - acc: 1.0000\n",
      "Epoch 191/200\n",
      "11/11 - 0s - loss: 0.1387 - acc: 1.0000\n",
      "Epoch 192/200\n",
      "11/11 - 0s - loss: 0.1365 - acc: 1.0000\n",
      "Epoch 193/200\n",
      "11/11 - 0s - loss: 0.1343 - acc: 1.0000\n",
      "Epoch 194/200\n",
      "11/11 - 0s - loss: 0.1322 - acc: 1.0000\n",
      "Epoch 195/200\n",
      "11/11 - 0s - loss: 0.1301 - acc: 1.0000\n",
      "Epoch 196/200\n",
      "11/11 - 0s - loss: 0.1281 - acc: 1.0000\n",
      "Epoch 197/200\n",
      "11/11 - 0s - loss: 0.1261 - acc: 1.0000\n",
      "Epoch 198/200\n",
      "11/11 - 0s - loss: 0.1241 - acc: 1.0000\n",
      "Epoch 199/200\n",
      "11/11 - 0s - loss: 0.1222 - acc: 1.0000\n",
      "Epoch 200/200\n",
      "11/11 - 0s - loss: 0.1203 - acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# 모델 생성방식은 CNN과 동일 - Sequential 방식\n",
    "# 추후 tensorflow를 이용해 모델 내부를 뜯어보면 매우 복잡해질 수 있음\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=max_len-1)) # 레이블을 분리하였으므로 이제 X의 길이는 5\n",
    "model.add(SimpleRNN(32))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "hist = model.fit(X, y, epochs=200, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 출력 확인\n",
    "def sentence_generation(model, t, current_word, n): # 모델, 토크나이저, 현재 단어, 반복할 횟수\n",
    "    init_word = current_word # 처음 들어온 단어도 마지막에 같이 출력하기위해 저장\n",
    "    sentence = ''\n",
    "    for _ in range(n): # n번 반복\n",
    "        encoded = t.texts_to_sequences([current_word])[0] # 현재 단어에 대한 정수 인코딩\n",
    "        encoded = pad_sequences([encoded], maxlen=5, padding='pre') # 데이터에 대한 패딩\n",
    "        result = model.predict_classes(encoded, verbose=0)\n",
    "    # 입력한 X(현재 단어)에 대해서 Y를 예측하고 Y(예측한 단어)를 result에 저장.\n",
    "        for word, index in t.word_index.items(): \n",
    "            if index == result: # 만약 예측한 단어와 인덱스와 동일한 단어가 있다면\n",
    "                break # 해당 단어가 예측 단어이므로 break\n",
    "        current_word = current_word + ' '  + word # 현재 단어 + ' ' + 예측 단어를 현재 단어로 변경\n",
    "        sentence = sentence + ' ' + word # 예측 단어를 문장에 저장\n",
    "    # for문이므로 이 행동을 다시 반복\n",
    "    sentence = init_word + sentence\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected embedding_2_input to have shape (23,) but got array with shape (5,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-8721627508cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# '경마장에' 라는 단어 뒤에는 총 4개의 단어가 있으므로 4번 예측\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_generation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'경마장에'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-61-2be813dcd518>\u001b[0m in \u001b[0;36msentence_generation\u001b[1;34m(model, t, current_word, n)\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mencoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurrent_word\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# 현재 단어에 대한 정수 인코딩\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mencoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mencoded\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pre'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 데이터에 대한 패딩\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;31m# 입력한 X(현재 단어)에 대해서 Y를 예측하고 Y(예측한 단어)를 result에 저장.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\data37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36mpredict_classes\u001b[1;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marray\u001b[0m \u001b[0mof\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m     \"\"\"\n\u001b[1;32m--> 326\u001b[1;33m     \u001b[0mproba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    327\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mproba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mproba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\data37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    906\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 908\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    909\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    910\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\data37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    714\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_or_infer_batch_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m     x, _, _ = model._standardize_user_data(\n\u001b[1;32m--> 716\u001b[1;33m         x, check_steps=True, steps_name='steps', steps=steps)\n\u001b[0m\u001b[0;32m    717\u001b[0m     return predict_loop(\n\u001b[0;32m    718\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\data37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2469\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2470\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2471\u001b[1;33m           exception_prefix='input')\n\u001b[0m\u001b[0;32m   2472\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2473\u001b[0m     \u001b[1;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\data37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    570\u001b[0m                              \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m                              str(data_shape))\n\u001b[0m\u001b[0;32m    573\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected embedding_2_input to have shape (23,) but got array with shape (5,)"
     ]
    }
   ],
   "source": [
    "# '경마장에' 라는 단어 뒤에는 총 4개의 단어가 있으므로 4번 예측\n",
    "print(sentence_generation(model, t, '경마장에', 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "그의 말이 법이다\n"
     ]
    }
   ],
   "source": [
    "print(sentence_generation(model, t, '그의', 2)) # 2번 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가는 말이 고와야 오는 말이 곱다\n"
     ]
    }
   ],
   "source": [
    "print(sentence_generation(model, t, '가는', 5)) # 5번 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "싸피 말이 말이 고와야 오는 말이\n",
      "his 말이 말이\n"
     ]
    }
   ],
   "source": [
    "# 생소한 단어에 대해서는 이상하게 동작하는것 확인 가능\n",
    "print(sentence_generation(model, t, '싸피', 5))\n",
    "print(sentence_generation(model, t, 'his',2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\ssafy\\\\s02p22a401\\\\doc\\\\RNN_summary'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from string import punctuation\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>articleID</th>\n",
       "      <th>articleWordCount</th>\n",
       "      <th>byline</th>\n",
       "      <th>documentType</th>\n",
       "      <th>headline</th>\n",
       "      <th>keywords</th>\n",
       "      <th>multimedia</th>\n",
       "      <th>newDesk</th>\n",
       "      <th>printPage</th>\n",
       "      <th>pubDate</th>\n",
       "      <th>sectionName</th>\n",
       "      <th>snippet</th>\n",
       "      <th>source</th>\n",
       "      <th>typeOfMaterial</th>\n",
       "      <th>webURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5adf6684068401528a2aa69b</td>\n",
       "      <td>781</td>\n",
       "      <td>By JOHN BRANCH</td>\n",
       "      <td>article</td>\n",
       "      <td>Former N.F.L. Cheerleaders’ Settlement Offer: ...</td>\n",
       "      <td>['Workplace Hazards and Violations', 'Football...</td>\n",
       "      <td>68</td>\n",
       "      <td>Sports</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-04-24 17:16:49</td>\n",
       "      <td>Pro Football</td>\n",
       "      <td>“I understand that they could meet with us, pa...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2018/04/24/sports/foot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5adf653f068401528a2aa697</td>\n",
       "      <td>656</td>\n",
       "      <td>By LISA FRIEDMAN</td>\n",
       "      <td>article</td>\n",
       "      <td>E.P.A. to Unveil a New Rule. Its Effect: Less ...</td>\n",
       "      <td>['Environmental Protection Agency', 'Pruitt, S...</td>\n",
       "      <td>68</td>\n",
       "      <td>Climate</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-04-24 17:11:21</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>The agency plans to publish a new regulation T...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2018/04/24/climate/epa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5adf4626068401528a2aa628</td>\n",
       "      <td>2427</td>\n",
       "      <td>By PETE WELLS</td>\n",
       "      <td>article</td>\n",
       "      <td>The New Noma, Explained</td>\n",
       "      <td>['Restaurants', 'Noma (Copenhagen, Restaurant)...</td>\n",
       "      <td>66</td>\n",
       "      <td>Dining</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-04-24 14:58:44</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>What’s it like to eat at the second incarnatio...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2018/04/24/dining/noma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5adf40d2068401528a2aa619</td>\n",
       "      <td>626</td>\n",
       "      <td>By JULIE HIRSCHFELD DAVIS and PETER BAKER</td>\n",
       "      <td>article</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>['Macron, Emmanuel (1977- )', 'Trump, Donald J...</td>\n",
       "      <td>68</td>\n",
       "      <td>Washington</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-04-24 14:35:57</td>\n",
       "      <td>Europe</td>\n",
       "      <td>President Trump welcomed President Emmanuel Ma...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2018/04/24/world/europ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5adf3d64068401528a2aa60f</td>\n",
       "      <td>815</td>\n",
       "      <td>By IAN AUSTEN and DAN BILEFSKY</td>\n",
       "      <td>article</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>['Toronto, Ontario, Attack (April, 2018)', 'Mu...</td>\n",
       "      <td>68</td>\n",
       "      <td>Foreign</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-04-24 14:21:21</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Alek Minassian, 25, a resident of Toronto’s Ri...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2018/04/24/world/canad...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  articleID  articleWordCount  \\\n",
       "0  5adf6684068401528a2aa69b               781   \n",
       "1  5adf653f068401528a2aa697               656   \n",
       "2  5adf4626068401528a2aa628              2427   \n",
       "3  5adf40d2068401528a2aa619               626   \n",
       "4  5adf3d64068401528a2aa60f               815   \n",
       "\n",
       "                                      byline documentType  \\\n",
       "0                             By JOHN BRANCH      article   \n",
       "1                           By LISA FRIEDMAN      article   \n",
       "2                              By PETE WELLS      article   \n",
       "3  By JULIE HIRSCHFELD DAVIS and PETER BAKER      article   \n",
       "4             By IAN AUSTEN and DAN BILEFSKY      article   \n",
       "\n",
       "                                            headline  \\\n",
       "0  Former N.F.L. Cheerleaders’ Settlement Offer: ...   \n",
       "1  E.P.A. to Unveil a New Rule. Its Effect: Less ...   \n",
       "2                            The New Noma, Explained   \n",
       "3                                            Unknown   \n",
       "4                                            Unknown   \n",
       "\n",
       "                                            keywords  multimedia     newDesk  \\\n",
       "0  ['Workplace Hazards and Violations', 'Football...          68      Sports   \n",
       "1  ['Environmental Protection Agency', 'Pruitt, S...          68     Climate   \n",
       "2  ['Restaurants', 'Noma (Copenhagen, Restaurant)...          66      Dining   \n",
       "3  ['Macron, Emmanuel (1977- )', 'Trump, Donald J...          68  Washington   \n",
       "4  ['Toronto, Ontario, Attack (April, 2018)', 'Mu...          68     Foreign   \n",
       "\n",
       "   printPage              pubDate   sectionName  \\\n",
       "0          0  2018-04-24 17:16:49  Pro Football   \n",
       "1          0  2018-04-24 17:11:21       Unknown   \n",
       "2          0  2018-04-24 14:58:44       Unknown   \n",
       "3          0  2018-04-24 14:35:57        Europe   \n",
       "4          0  2018-04-24 14:21:21        Canada   \n",
       "\n",
       "                                             snippet              source  \\\n",
       "0  “I understand that they could meet with us, pa...  The New York Times   \n",
       "1  The agency plans to publish a new regulation T...  The New York Times   \n",
       "2  What’s it like to eat at the second incarnatio...  The New York Times   \n",
       "3  President Trump welcomed President Emmanuel Ma...  The New York Times   \n",
       "4  Alek Minassian, 25, a resident of Toronto’s Ri...  The New York Times   \n",
       "\n",
       "  typeOfMaterial                                             webURL  \n",
       "0           News  https://www.nytimes.com/2018/04/24/sports/foot...  \n",
       "1           News  https://www.nytimes.com/2018/04/24/climate/epa...  \n",
       "2           News  https://www.nytimes.com/2018/04/24/dining/noma...  \n",
       "3           News  https://www.nytimes.com/2018/04/24/world/europ...  \n",
       "4           News  https://www.nytimes.com/2018/04/24/world/canad...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 뉴욕타임스 2018년 기사 데이터\n",
    "df=pd.read_csv('nyt-comments/ArticlesApril2018.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Former N.F.L. Cheerleaders’ Settlement Offer: $1 and a Meeting With Goodell',\n",
       " 'E.P.A. to Unveil a New Rule. Its Effect: Less Science in Policymaking.',\n",
       " 'The New Noma, Explained',\n",
       " 'Unknown',\n",
       " 'Unknown']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Noise 제거\n",
    "headline = [] # 리스트 선언\n",
    "headline.extend(list(df.headline.values)) # 헤드라인의 값들을 리스트로 저장\n",
    "headline[:5] # 상위 5개만 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 샘플의 개수 : 1214\n"
     ]
    }
   ],
   "source": [
    "print('총 샘플의 개수 : {}'.format(len(headline))) # 현재 샘플의 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "노이즈값 제거 후 샘플의 개수 : 1214\n"
     ]
    }
   ],
   "source": [
    "headline = [n for n in headline if n != \"Unknown\"] # Unknown 값을 가진 샘플 제거\n",
    "print('노이즈값 제거 후 샘플의 개수 : {}'.format(len(headline))) # 제거 후 샘플의 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['former nfl cheerleaders settlement offer 1 and a meeting with goodell',\n",
       " 'epa to unveil a new rule its effect less science in policymaking',\n",
       " 'the new noma explained',\n",
       " 'how a bag of texas dirt  became a times tradition',\n",
       " 'is school a place for selfexpression']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#데이터 전처리 - 구두점,문장부호 제거, lower 적용\n",
    "def repreprocessing(s):\n",
    "    s=s.encode(\"utf8\").decode(\"ascii\",'ignore')\n",
    "    return ''.join(c for c in s if c not in punctuation).lower() # 구두점 제거와 동시에 소문자화\n",
    "\n",
    "text = [repreprocessing(x) for x in headline]\n",
    "text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 3494\n"
     ]
    }
   ],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts(text)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "print('단어 집합의 크기 : %d' % vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'a': 2,\n",
       " 'to': 3,\n",
       " 'of': 4,\n",
       " 'in': 5,\n",
       " 'for': 6,\n",
       " 'and': 7,\n",
       " 'is': 8,\n",
       " 'on': 9,\n",
       " 'with': 10,\n",
       " 'trump': 11,\n",
       " 'as': 12,\n",
       " 'at': 13,\n",
       " 'new': 14,\n",
       " 'how': 15,\n",
       " 'from': 16,\n",
       " 'it': 17,\n",
       " 'an': 18,\n",
       " 'that': 19,\n",
       " 'be': 20,\n",
       " 'season': 21,\n",
       " 'us': 22,\n",
       " 'you': 23,\n",
       " 'its': 24,\n",
       " 'what': 25,\n",
       " 'episode': 26,\n",
       " 'can': 27,\n",
       " 'your': 28,\n",
       " 'not': 29,\n",
       " 'he': 30,\n",
       " 'now': 31,\n",
       " 'his': 32,\n",
       " 'are': 33,\n",
       " 'teaching': 34,\n",
       " 'war': 35,\n",
       " 'out': 36,\n",
       " 'no': 37,\n",
       " 'was': 38,\n",
       " 'by': 39,\n",
       " 'trumps': 40,\n",
       " 'has': 41,\n",
       " 'over': 42,\n",
       " 'may': 43,\n",
       " 'into': 44,\n",
       " 'why': 45,\n",
       " 'more': 46,\n",
       " 'we': 47,\n",
       " 'who': 48,\n",
       " 'about': 49,\n",
       " 'recap': 50,\n",
       " 'activities': 51,\n",
       " '1': 52,\n",
       " 'just': 53,\n",
       " 'do': 54,\n",
       " 'women': 55,\n",
       " 'when': 56,\n",
       " 'syria': 57,\n",
       " 'trade': 58,\n",
       " 'i': 59,\n",
       " '2': 60,\n",
       " 'or': 61,\n",
       " 'will': 62,\n",
       " 'this': 63,\n",
       " 'have': 64,\n",
       " 'president': 65,\n",
       " 'but': 66,\n",
       " 'home': 67,\n",
       " 'up': 68,\n",
       " 'long': 69,\n",
       " 'one': 70,\n",
       " 'off': 71,\n",
       " 'facebook': 72,\n",
       " 'house': 73,\n",
       " 'gop': 74,\n",
       " 'our': 75,\n",
       " 'case': 76,\n",
       " 'they': 77,\n",
       " 'life': 78,\n",
       " 'end': 79,\n",
       " 'right': 80,\n",
       " 'some': 81,\n",
       " 'big': 82,\n",
       " 'dead': 83,\n",
       " 'power': 84,\n",
       " 'say': 85,\n",
       " 'white': 86,\n",
       " 'after': 87,\n",
       " 'still': 88,\n",
       " 'north': 89,\n",
       " 'my': 90,\n",
       " 'dont': 91,\n",
       " 'need': 92,\n",
       " 'race': 93,\n",
       " 'own': 94,\n",
       " 'against': 95,\n",
       " 'here': 96,\n",
       " 'should': 97,\n",
       " 'border': 98,\n",
       " 'former': 99,\n",
       " 'epa': 100,\n",
       " 'battle': 101,\n",
       " 'mr': 102,\n",
       " 'too': 103,\n",
       " 'their': 104,\n",
       " 'plan': 105,\n",
       " '3': 106,\n",
       " 'china': 107,\n",
       " 'real': 108,\n",
       " 'were': 109,\n",
       " 'her': 110,\n",
       " 'russia': 111,\n",
       " 'art': 112,\n",
       " 'good': 113,\n",
       " 'then': 114,\n",
       " 'like': 115,\n",
       " 'pay': 116,\n",
       " 'back': 117,\n",
       " 'get': 118,\n",
       " 'love': 119,\n",
       " 'says': 120,\n",
       " 'officials': 121,\n",
       " 'fight': 122,\n",
       " 'tariffs': 123,\n",
       " 'pruitt': 124,\n",
       " 'democrats': 125,\n",
       " 'black': 126,\n",
       " 'man': 127,\n",
       " 'men': 128,\n",
       " 'help': 129,\n",
       " 'never': 130,\n",
       " 'york': 131,\n",
       " 'comey': 132,\n",
       " 'chief': 133,\n",
       " 'metoo': 134,\n",
       " 'work': 135,\n",
       " 'place': 136,\n",
       " 'could': 137,\n",
       " 'past': 138,\n",
       " 'years': 139,\n",
       " 'rights': 140,\n",
       " 'first': 141,\n",
       " 'money': 142,\n",
       " 'save': 143,\n",
       " 'going': 144,\n",
       " 'all': 145,\n",
       " 'way': 146,\n",
       " 'political': 147,\n",
       " 'fear': 148,\n",
       " 'next': 149,\n",
       " 'fire': 150,\n",
       " 'party': 151,\n",
       " 'me': 152,\n",
       " 'becomes': 153,\n",
       " '8': 154,\n",
       " 'better': 155,\n",
       " 'old': 156,\n",
       " 'dr': 157,\n",
       " 'king': 158,\n",
       " 'homes': 159,\n",
       " 'ryan': 160,\n",
       " 'tax': 161,\n",
       " 'if': 162,\n",
       " 'than': 163,\n",
       " 'americans': 164,\n",
       " 'rules': 165,\n",
       " 'police': 166,\n",
       " 'school': 167,\n",
       " 'leaders': 168,\n",
       " 'korea': 169,\n",
       " 'there': 170,\n",
       " 'top': 171,\n",
       " 'court': 172,\n",
       " 'state': 173,\n",
       " '10': 174,\n",
       " 'lower': 175,\n",
       " 'states': 176,\n",
       " 'whats': 177,\n",
       " 'use': 178,\n",
       " 'cancer': 179,\n",
       " 'britain': 180,\n",
       " 'wont': 181,\n",
       " 'time': 182,\n",
       " 'america': 183,\n",
       " 'history': 184,\n",
       " 'where': 185,\n",
       " 'lead': 186,\n",
       " 'left': 187,\n",
       " 'plans': 188,\n",
       " 'talk': 189,\n",
       " 'crisis': 190,\n",
       " 'two': 191,\n",
       " 'tells': 192,\n",
       " 'trust': 193,\n",
       " 'nuclear': 194,\n",
       " 'children': 195,\n",
       " 'million': 196,\n",
       " 'make': 197,\n",
       " 'leader': 198,\n",
       " 'others': 199,\n",
       " 'attack': 200,\n",
       " 'people': 201,\n",
       " 'another': 202,\n",
       " '6': 203,\n",
       " 'world': 204,\n",
       " 'day': 205,\n",
       " 'car': 206,\n",
       " 'young': 207,\n",
       " 'little': 208,\n",
       " 'california': 209,\n",
       " 'crash': 210,\n",
       " 'last': 211,\n",
       " 'book': 212,\n",
       " 'death': 213,\n",
       " 'gun': 214,\n",
       " 'texas': 215,\n",
       " 'hes': 216,\n",
       " 'sex': 217,\n",
       " 'abuse': 218,\n",
       " 'find': 219,\n",
       " 'truth': 220,\n",
       " 'arizona': 221,\n",
       " 'american': 222,\n",
       " 'pompeo': 223,\n",
       " 'change': 224,\n",
       " 'city': 225,\n",
       " 'kim': 226,\n",
       " 'so': 227,\n",
       " 'music': 228,\n",
       " 'risks': 229,\n",
       " 'being': 230,\n",
       " 'billions': 231,\n",
       " '5': 232,\n",
       " 'family': 233,\n",
       " 'missing': 234,\n",
       " 'gaza': 235,\n",
       " 'vs': 236,\n",
       " 'senate': 237,\n",
       " 'heart': 238,\n",
       " 'justice': 239,\n",
       " 'killing': 240,\n",
       " 'cia': 241,\n",
       " 'come': 242,\n",
       " 'shows': 243,\n",
       " 'turning': 244,\n",
       " 'schools': 245,\n",
       " 'parents': 246,\n",
       " 'law': 247,\n",
       " 'legal': 248,\n",
       " 'economy': 249,\n",
       " 'strike': 250,\n",
       " 'him': 251,\n",
       " 'threat': 252,\n",
       " 'before': 253,\n",
       " 'takes': 254,\n",
       " 'night': 255,\n",
       " 'mueller': 256,\n",
       " 'close': 257,\n",
       " 'cut': 258,\n",
       " 'dream': 259,\n",
       " 'face': 260,\n",
       " 'friends': 261,\n",
       " 'does': 262,\n",
       " 'game': 263,\n",
       " 'dear': 264,\n",
       " 'boss': 265,\n",
       " 'baby': 266,\n",
       " 'kings': 267,\n",
       " 'jail': 268,\n",
       " 'nfl': 269,\n",
       " 'jimmy': 270,\n",
       " 'word': 271,\n",
       " 'hot': 272,\n",
       " 'turns': 273,\n",
       " 'gap': 274,\n",
       " 'got': 275,\n",
       " 'hope': 276,\n",
       " 'making': 277,\n",
       " 'overlooked': 278,\n",
       " 'push': 279,\n",
       " 'high': 280,\n",
       " 'deal': 281,\n",
       " 'heck': 282,\n",
       " 'live': 283,\n",
       " 'families': 284,\n",
       " 'wrong': 285,\n",
       " '2018': 286,\n",
       " 'fix': 287,\n",
       " 'lawyers': 288,\n",
       " 'koreas': 289,\n",
       " 'choose': 290,\n",
       " 'public': 291,\n",
       " 'cuomo': 292,\n",
       " 'steel': 293,\n",
       " 'open': 294,\n",
       " 'scott': 295,\n",
       " 'beyond': 296,\n",
       " 'variety': 297,\n",
       " 'ethics': 298,\n",
       " 'files': 299,\n",
       " 'dept': 300,\n",
       " 'let': 301,\n",
       " 'inside': 302,\n",
       " 'director': 303,\n",
       " 'far': 304,\n",
       " 'side': 305,\n",
       " 'rupauls': 306,\n",
       " 'drag': 307,\n",
       " 'fence': 308,\n",
       " 'sanctions': 309,\n",
       " 'want': 310,\n",
       " 'global': 311,\n",
       " 'great': 312,\n",
       " 'fish': 313,\n",
       " 'market': 314,\n",
       " 'team': 315,\n",
       " 'eat': 316,\n",
       " 'problem': 317,\n",
       " 'miracle': 318,\n",
       " 'tied': 319,\n",
       " 'hours': 320,\n",
       " 'working': 321,\n",
       " 'other': 322,\n",
       " 'yet': 323,\n",
       " 'call': 324,\n",
       " 'allies': 325,\n",
       " 'privacy': 326,\n",
       " 'data': 327,\n",
       " 'experts': 328,\n",
       " 'go': 329,\n",
       " 'met': 330,\n",
       " 'brooklyn': 331,\n",
       " 'didnt': 332,\n",
       " 'russian': 333,\n",
       " 'trial': 334,\n",
       " 'apply': 335,\n",
       " 'college': 336,\n",
       " 'many': 337,\n",
       " '4': 338,\n",
       " 'security': 339,\n",
       " 'year': 340,\n",
       " 'tale': 341,\n",
       " 'social': 342,\n",
       " 'control': 343,\n",
       " 'been': 344,\n",
       " 'hit': 345,\n",
       " 'early': 346,\n",
       " 'behind': 347,\n",
       " 'match': 348,\n",
       " 'ok': 349,\n",
       " 'fears': 350,\n",
       " 'isis': 351,\n",
       " 'walking': 352,\n",
       " 'national': 353,\n",
       " 'presidency': 354,\n",
       " 'student': 355,\n",
       " 'second': 356,\n",
       " 'limits': 357,\n",
       " 'care': 358,\n",
       " 'era': 359,\n",
       " 'south': 360,\n",
       " 'guard': 361,\n",
       " 'rise': 362,\n",
       " 'edge': 363,\n",
       " 'hero': 364,\n",
       " 'tech': 365,\n",
       " 'secret': 366,\n",
       " 'storm': 367,\n",
       " 'gets': 368,\n",
       " 'watch': 369,\n",
       " 'weapons': 370,\n",
       " 'cheerleaders': 371,\n",
       " 'meeting': 372,\n",
       " 'less': 373,\n",
       " 'science': 374,\n",
       " 'dirt': 375,\n",
       " 'times': 376,\n",
       " 'looking': 377,\n",
       " 'win': 378,\n",
       " 'pope': 379,\n",
       " 'stuff': 380,\n",
       " 'wants': 381,\n",
       " 'ruling': 382,\n",
       " 'town': 383,\n",
       " 'cold': 384,\n",
       " 'behavior': 385,\n",
       " 'guns': 386,\n",
       " 'stand': 387,\n",
       " 'human': 388,\n",
       " 'economic': 389,\n",
       " 'tragedy': 390,\n",
       " 'paul': 391,\n",
       " 'them': 392,\n",
       " 'key': 393,\n",
       " 'down': 394,\n",
       " 'given': 395,\n",
       " 'urge': 396,\n",
       " 'kids': 397,\n",
       " 'westworld': 398,\n",
       " 'picture': 399,\n",
       " 'april': 400,\n",
       " '23': 401,\n",
       " 'subway': 402,\n",
       " 'lets': 403,\n",
       " 'impeachment': 404,\n",
       " 'feel': 405,\n",
       " 'told': 406,\n",
       " 'smile': 407,\n",
       " 'meet': 408,\n",
       " 'wild': 409,\n",
       " 'guide': 410,\n",
       " 'business': 411,\n",
       " 'republicans': 412,\n",
       " 'starbucks': 413,\n",
       " 'door': 414,\n",
       " 'heres': 415,\n",
       " 'punch': 416,\n",
       " 'air': 417,\n",
       " 'caution': 418,\n",
       " 'democratic': 419,\n",
       " 'seen': 420,\n",
       " 'west': 421,\n",
       " 'once': 422,\n",
       " 'marriage': 423,\n",
       " 'politics': 424,\n",
       " 'mission': 425,\n",
       " 'support': 426,\n",
       " 'extra': 427,\n",
       " 'stephen': 428,\n",
       " 'colbert': 429,\n",
       " 'doesnt': 430,\n",
       " 'marijuana': 431,\n",
       " 'reading': 432,\n",
       " 'flight': 433,\n",
       " 'navy': 434,\n",
       " 'veteran': 435,\n",
       " 'ban': 436,\n",
       " 'atlanta': 437,\n",
       " 'finally': 438,\n",
       " 'ready': 439,\n",
       " 'todays': 440,\n",
       " 'puzzle': 441,\n",
       " 'deputy': 442,\n",
       " 'gave': 443,\n",
       " 'teenagers': 444,\n",
       " 'training': 445,\n",
       " 'cant': 446,\n",
       " 'remember': 447,\n",
       " '36': 448,\n",
       " 'found': 449,\n",
       " 'look': 450,\n",
       " 'scientists': 451,\n",
       " 'drug': 452,\n",
       " 'start': 453,\n",
       " 'fit': 454,\n",
       " 'syrian': 455,\n",
       " 'israel': 456,\n",
       " 'bush': 457,\n",
       " 'tweets': 458,\n",
       " 'calling': 459,\n",
       " 'genius': 460,\n",
       " 'acrostic': 461,\n",
       " 'strikes': 462,\n",
       " 'americas': 463,\n",
       " 'pregnancy': 464,\n",
       " 'step': 465,\n",
       " 'washington': 466,\n",
       " 'body': 467,\n",
       " 'supreme': 468,\n",
       " 'best': 469,\n",
       " 'run': 470,\n",
       " 'late': 471,\n",
       " 'force': 472,\n",
       " 'france': 473,\n",
       " 'said': 474,\n",
       " 'raid': 475,\n",
       " 'report': 476,\n",
       " 'aide': 477,\n",
       " 'try': 478,\n",
       " 'line': 479,\n",
       " 'loss': 480,\n",
       " 'risk': 481,\n",
       " 'ask': 482,\n",
       " 'trevor': 483,\n",
       " 'noah': 484,\n",
       " 'become': 485,\n",
       " 'affair': 486,\n",
       " 'died': 487,\n",
       " 'travel': 488,\n",
       " 'pollution': 489,\n",
       " 'building': 490,\n",
       " 'details': 491,\n",
       " 'mike': 492,\n",
       " 'immigration': 493,\n",
       " 'much': 494,\n",
       " 'words': 495,\n",
       " 'press': 496,\n",
       " 'energy': 497,\n",
       " 'think': 498,\n",
       " 'kitchen': 499,\n",
       " 'teams': 500,\n",
       " 'roseanne': 501,\n",
       " 'matter': 502,\n",
       " 'warm': 503,\n",
       " 'cohen': 504,\n",
       " 'eye': 505,\n",
       " 'wait': 506,\n",
       " 'seized': 507,\n",
       " 'civil': 508,\n",
       " 'brain': 509,\n",
       " 'die': 510,\n",
       " 'bad': 511,\n",
       " 'island': 512,\n",
       " 'target': 513,\n",
       " 'michael': 514,\n",
       " 'complicated': 515,\n",
       " 'under': 516,\n",
       " 'test': 517,\n",
       " 'did': 518,\n",
       " 'talks': 519,\n",
       " 'put': 520,\n",
       " 'questions': 521,\n",
       " 'dies': 522,\n",
       " 'moves': 523,\n",
       " '15': 524,\n",
       " 'lives': 525,\n",
       " 'james': 526,\n",
       " 'gives': 527,\n",
       " 'warriors': 528,\n",
       " 'mainstream': 529,\n",
       " 'saudi': 530,\n",
       " 'loan': 531,\n",
       " 'puts': 532,\n",
       " 'star': 533,\n",
       " 'broken': 534,\n",
       " 'glass': 535,\n",
       " 'moments': 536,\n",
       " 'sick': 537,\n",
       " 'beat': 538,\n",
       " 'god': 539,\n",
       " 'mean': 540,\n",
       " 'spy': 541,\n",
       " 'immigrants': 542,\n",
       " 'kill': 543,\n",
       " 'shame': 544,\n",
       " 'maybe': 545,\n",
       " 'even': 546,\n",
       " 'land': 547,\n",
       " '14': 548,\n",
       " 'policy': 549,\n",
       " 'agent': 550,\n",
       " 'smart': 551,\n",
       " 'martin': 552,\n",
       " 'luther': 553,\n",
       " 'mind': 554,\n",
       " 'earth': 555,\n",
       " 'action': 556,\n",
       " 'iraq': 557,\n",
       " 'feeling': 558,\n",
       " 'nature': 559,\n",
       " 'church': 560,\n",
       " 'beware': 561,\n",
       " 'away': 562,\n",
       " 'point': 563,\n",
       " 'army': 564,\n",
       " 'va': 565,\n",
       " 'girls': 566,\n",
       " 'play': 567,\n",
       " 'made': 568,\n",
       " 'marathon': 569,\n",
       " 'lies': 570,\n",
       " 'turn': 571,\n",
       " 'fiction': 572,\n",
       " 'terror': 573,\n",
       " 'accept': 574,\n",
       " 'dying': 575,\n",
       " 'victims': 576,\n",
       " 'golden': 577,\n",
       " 'common': 578,\n",
       " 'near': 579,\n",
       " 'cosby': 580,\n",
       " 'revolt': 581,\n",
       " 'offer': 582,\n",
       " 'rule': 583,\n",
       " 'bag': 584,\n",
       " 'tradition': 585,\n",
       " 'failed': 586,\n",
       " 'utah': 587,\n",
       " 'few': 588,\n",
       " 'believe': 589,\n",
       " 'forced': 590,\n",
       " 'artists': 591,\n",
       " 'carter': 592,\n",
       " 'jersey': 593,\n",
       " 'quiz': 594,\n",
       " 'lifethreatening': 595,\n",
       " 'food': 596,\n",
       " 'choosing': 597,\n",
       " 'future': 598,\n",
       " 'gone': 599,\n",
       " 'panel': 600,\n",
       " 'bucks': 601,\n",
       " 'sidewalk': 602,\n",
       " 'van': 603,\n",
       " 'cuts': 604,\n",
       " 'apologies': 605,\n",
       " 'europes': 606,\n",
       " 'iran': 607,\n",
       " 'themselves': 608,\n",
       " 'avengers': 609,\n",
       " 'most': 610,\n",
       " 'movie': 611,\n",
       " 'taking': 612,\n",
       " 'mirror': 613,\n",
       " 'chancellor': 614,\n",
       " 'debate': 615,\n",
       " 'foods': 616,\n",
       " 'citys': 617,\n",
       " 'favorite': 618,\n",
       " 'cruelty': 619,\n",
       " 'robots': 620,\n",
       " 'coffee': 621,\n",
       " 'prince': 622,\n",
       " 'share': 623,\n",
       " 'tolerance': 624,\n",
       " 'stop': 625,\n",
       " 'migrants': 626,\n",
       " 'path': 627,\n",
       " 'aiding': 628,\n",
       " 'europe': 629,\n",
       " 'magic': 630,\n",
       " 'kind': 631,\n",
       " 'losing': 632,\n",
       " 'middle': 633,\n",
       " 'class': 634,\n",
       " 'youll': 635,\n",
       " 'sorry': 636,\n",
       " 'swamp': 637,\n",
       " 'columbia': 638,\n",
       " 'wages': 639,\n",
       " 'painfully': 640,\n",
       " 'wonkish': 641,\n",
       " 'forgotten': 642,\n",
       " 'survival': 643,\n",
       " 'decline': 644,\n",
       " 'mess': 645,\n",
       " 'fake': 646,\n",
       " 'set': 647,\n",
       " 'himself': 648,\n",
       " 'barely': 649,\n",
       " 'quit': 650,\n",
       " '70': 651,\n",
       " 'conspiracy': 652,\n",
       " 'pick': 653,\n",
       " 'wells': 654,\n",
       " 'fargo': 655,\n",
       " 'mayhem': 656,\n",
       " 'fields': 657,\n",
       " '50': 658,\n",
       " 'lack': 659,\n",
       " 'cynthia': 660,\n",
       " 'nixon': 661,\n",
       " 'along': 662,\n",
       " 'promise': 663,\n",
       " 'testing': 664,\n",
       " 'candidates': 665,\n",
       " 'pruitts': 666,\n",
       " 'reforms': 667,\n",
       " 'selling': 668,\n",
       " 'know': 669,\n",
       " 'runners': 670,\n",
       " 'tarot': 671,\n",
       " 'card': 672,\n",
       " 'dress': 673,\n",
       " 'pilot': 674,\n",
       " 'nerves': 675,\n",
       " 'letting': 676,\n",
       " 'al': 677,\n",
       " 'equal': 678,\n",
       " 'returns': 679,\n",
       " 'snake': 680,\n",
       " 'oil': 681,\n",
       " 'betrayed': 682,\n",
       " 'nation': 683,\n",
       " 'fashion': 684,\n",
       " 'lay': 685,\n",
       " 'fine': 686,\n",
       " 'fraud': 687,\n",
       " 'again': 688,\n",
       " 'exfbi': 689,\n",
       " 'sent': 690,\n",
       " 'name': 691,\n",
       " 'sea': 692,\n",
       " 'stands': 693,\n",
       " 'spur': 694,\n",
       " 'means': 695,\n",
       " 'progressive': 696,\n",
       " 'standardized': 697,\n",
       " 'tests': 698,\n",
       " 'antibias': 699,\n",
       " 'goal': 700,\n",
       " 'gentrification': 701,\n",
       " 'shine': 702,\n",
       " 'road': 703,\n",
       " 'deportation': 704,\n",
       " 'voice': 705,\n",
       " 'crazy': 706,\n",
       " 'warming': 707,\n",
       " 'interview': 708,\n",
       " 'launches': 709,\n",
       " 'allout': 710,\n",
       " 'barbara': 711,\n",
       " 'ill': 712,\n",
       " 'treatment': 713,\n",
       " 'blasts': 714,\n",
       " 'slippery': 715,\n",
       " 'around': 716,\n",
       " 'office': 717,\n",
       " 'todo': 718,\n",
       " 'list': 719,\n",
       " 'housing': 720,\n",
       " 'leaves': 721,\n",
       " 'pinch': 722,\n",
       " 'took': 723,\n",
       " 'assad': 724,\n",
       " 'library': 725,\n",
       " 'payment': 726,\n",
       " 'snarl': 727,\n",
       " 'makes': 728,\n",
       " '87': 729,\n",
       " 'comedy': 730,\n",
       " 'rising': 731,\n",
       " 'stars': 732,\n",
       " 'alike': 733,\n",
       " 'coal': 734,\n",
       " 'lobbyist': 735,\n",
       " 'crossword': 736,\n",
       " 'animals': 737,\n",
       " 'whos': 738,\n",
       " 'pentagon': 739,\n",
       " 'signs': 740,\n",
       " 'childhood': 741,\n",
       " 'courts': 742,\n",
       " 'shift': 743,\n",
       " 'saying': 744,\n",
       " 'scandal': 745,\n",
       " 'sticker': 746,\n",
       " 'shock': 747,\n",
       " 'teacher': 748,\n",
       " 'walkouts': 749,\n",
       " 'grip': 750,\n",
       " 'red': 751,\n",
       " 'speaker': 752,\n",
       " 'leave': 753,\n",
       " 'british': 754,\n",
       " 'mexico': 755,\n",
       " 'wish': 756,\n",
       " 'alive': 757,\n",
       " 'longer': 758,\n",
       " 'moscow': 759,\n",
       " 'believes': 760,\n",
       " 'beef': 761,\n",
       " 'stew': 762,\n",
       " 'chinese': 763,\n",
       " 'partner': 764,\n",
       " 'tabloid': 765,\n",
       " 'catches': 766,\n",
       " 'facebooks': 767,\n",
       " 'lot': 768,\n",
       " 'health': 769,\n",
       " 'putin': 770,\n",
       " 'india': 771,\n",
       " 'these': 772,\n",
       " 'begin': 773,\n",
       " 'protest': 774,\n",
       " 'florida': 775,\n",
       " 'colorado': 776,\n",
       " 'thats': 777,\n",
       " 'hopes': 778,\n",
       " 'access': 779,\n",
       " 'hollywood': 780,\n",
       " 'tape': 781,\n",
       " 'focus': 782,\n",
       " 'fbi': 783,\n",
       " 'general': 784,\n",
       " 'leak': 785,\n",
       " 'spring': 786,\n",
       " 'gut': 787,\n",
       " 'advice': 788,\n",
       " 'risotto': 789,\n",
       " 'simple': 790,\n",
       " 'during': 791,\n",
       " 'week': 792,\n",
       " 'ball': 793,\n",
       " 'gluten': 794,\n",
       " 'free': 795,\n",
       " 'seattle': 796,\n",
       " 'exhusband': 797,\n",
       " 'dilemma': 798,\n",
       " 'betting': 799,\n",
       " 'governors': 800,\n",
       " 'collector': 801,\n",
       " 'thriving': 802,\n",
       " 'modern': 803,\n",
       " 'count': 804,\n",
       " 'thousands': 805,\n",
       " 'interest': 806,\n",
       " 'alaska': 807,\n",
       " 'republican': 808,\n",
       " 'midterm': 809,\n",
       " 'elections': 810,\n",
       " 'pose': 811,\n",
       " 'worry': 812,\n",
       " 'view': 813,\n",
       " 'fascism': 814,\n",
       " 'without': 815,\n",
       " 'spending': 816,\n",
       " 'signals': 817,\n",
       " 'boys': 818,\n",
       " 'learning': 819,\n",
       " 'move': 820,\n",
       " 'fast': 821,\n",
       " 'governor': 822,\n",
       " 'those': 823,\n",
       " 'aging': 824,\n",
       " 'warns': 825,\n",
       " 'media': 826,\n",
       " 'exercise': 827,\n",
       " 'allergic': 828,\n",
       " 'signatures': 829,\n",
       " 'military': 830,\n",
       " 'disaster': 831,\n",
       " 'embrace': 832,\n",
       " 'youre': 833,\n",
       " 'defying': 834,\n",
       " 'toll': 835,\n",
       " 'humans': 836,\n",
       " 'store': 837,\n",
       " 'dogs': 838,\n",
       " 'client': 839,\n",
       " 'reports': 840,\n",
       " 'fired': 841,\n",
       " 'coming': 842,\n",
       " 'lavish': 843,\n",
       " 'zone': 844,\n",
       " 'called': 845,\n",
       " 'options': 846,\n",
       " 'paid': 847,\n",
       " 'ice': 848,\n",
       " 'adviser': 849,\n",
       " 'calls': 850,\n",
       " 'polar': 851,\n",
       " 'bears': 852,\n",
       " 'online': 853,\n",
       " 'bust': 854,\n",
       " 'bars': 855,\n",
       " 'private': 856,\n",
       " 'fans': 857,\n",
       " 'remains': 858,\n",
       " 'funny': 859,\n",
       " 'daughter': 860,\n",
       " 'offered': 861,\n",
       " 'massacre': 862,\n",
       " 'prison': 863,\n",
       " 'votes': 864,\n",
       " 'very': 865,\n",
       " 'outsiders': 866,\n",
       " 'gender': 867,\n",
       " 'scrutiny': 868,\n",
       " 'trail': 869,\n",
       " 'nightmare': 870,\n",
       " 'species': 871,\n",
       " 'fuels': 872,\n",
       " 'jacket': 873,\n",
       " 'hair': 874,\n",
       " '81': 875,\n",
       " 'foreign': 876,\n",
       " '9': 877,\n",
       " 'arabian': 878,\n",
       " 'official': 879,\n",
       " 'afghan': 880,\n",
       " 'enters': 881,\n",
       " 'journalism': 882,\n",
       " 'later': 883,\n",
       " 'cry': 884,\n",
       " 'vows': 885,\n",
       " 'price': 886,\n",
       " 'raising': 887,\n",
       " 'song': 888,\n",
       " 'gallery': 889,\n",
       " 'hockey': 890,\n",
       " 'fox': 891,\n",
       " 'fiery': 892,\n",
       " 'chemical': 893,\n",
       " 'hasty': 894,\n",
       " 'viral': 895,\n",
       " 'voters': 896,\n",
       " 'arlee': 897,\n",
       " 'arent': 898,\n",
       " 'friday': 899,\n",
       " 'something': 900,\n",
       " 'every': 901,\n",
       " 'buying': 902,\n",
       " 'ad': 903,\n",
       " 'asparagus': 904,\n",
       " 'blame': 905,\n",
       " 'democracy': 906,\n",
       " 'bracing': 907,\n",
       " 'blood': 908,\n",
       " 'estate': 909,\n",
       " 'jobs': 910,\n",
       " 'raised': 911,\n",
       " 'islam': 912,\n",
       " 'yorks': 913,\n",
       " 'give': 914,\n",
       " 'blast': 915,\n",
       " 'small': 916,\n",
       " 'forces': 917,\n",
       " 'tight': 918,\n",
       " 'labor': 919,\n",
       " 'manhattan': 920,\n",
       " 'intellectual': 921,\n",
       " 'tiny': 922,\n",
       " 'haunting': 923,\n",
       " 'thoughts': 924,\n",
       " 'misconduct': 925,\n",
       " 'usual': 926,\n",
       " 'lisa': 927,\n",
       " 'evil': 928,\n",
       " 'springs': 929,\n",
       " 'stress': 930,\n",
       " 'monkeys': 931,\n",
       " 'gay': 932,\n",
       " 'safety': 933,\n",
       " 'crashes': 934,\n",
       " 'queen': 935,\n",
       " 'indexes': 936,\n",
       " 'indian': 937,\n",
       " 'dreams': 938,\n",
       " 'role': 939,\n",
       " 'macrons': 940,\n",
       " 'locals': 941,\n",
       " 'shot': 942,\n",
       " 'whims': 943,\n",
       " 'sees': 944,\n",
       " 'makers': 945,\n",
       " 'pipe': 946,\n",
       " 'slow': 947,\n",
       " 'quiet': 948,\n",
       " 'ohio': 949,\n",
       " 'primary': 950,\n",
       " 'readers': 951,\n",
       " 'universe': 952,\n",
       " 'older': 953,\n",
       " 'well': 954,\n",
       " 'rents': 955,\n",
       " 'saving': 956,\n",
       " 'worship': 957,\n",
       " 'detail': 958,\n",
       " 'project': 959,\n",
       " 'dissent': 960,\n",
       " 'kennedy': 961,\n",
       " 'assassination': 962,\n",
       " 'edges': 963,\n",
       " 'brazil': 964,\n",
       " 'sports': 965,\n",
       " 'dad': 966,\n",
       " 'amazon': 967,\n",
       " 'trauma': 968,\n",
       " 'anxiety': 969,\n",
       " '100': 970,\n",
       " 'endless': 971,\n",
       " 'video': 972,\n",
       " 'legacy': 973,\n",
       " 'candidate': 974,\n",
       " 'shooting': 975,\n",
       " 'silicon': 976,\n",
       " 'valley': 977,\n",
       " 'light': 978,\n",
       " 'chose': 979,\n",
       " 'heated': 980,\n",
       " 'leads': 981,\n",
       " 'israelis': 982,\n",
       " 'lone': 983,\n",
       " 'journalist': 984,\n",
       " 'sexual': 985,\n",
       " 'assault': 986,\n",
       " 'joke': 987,\n",
       " 'bronx': 988,\n",
       " 'necessary': 989,\n",
       " 'search': 990,\n",
       " 'nights': 991,\n",
       " 'giants': 992,\n",
       " 'streets': 993,\n",
       " 'stocks': 994,\n",
       " 'see': 995,\n",
       " 'teachers': 996,\n",
       " 'spreads': 997,\n",
       " 'african': 998,\n",
       " 'only': 999,\n",
       " 'hard': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.word_index #전치사, 관사 등의 문법단어는 그대로 살아있음 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[99, 269],\n",
       " [99, 269, 371],\n",
       " [99, 269, 371, 1115],\n",
       " [99, 269, 371, 1115, 582],\n",
       " [99, 269, 371, 1115, 582, 52],\n",
       " [99, 269, 371, 1115, 582, 52, 7],\n",
       " [99, 269, 371, 1115, 582, 52, 7, 2],\n",
       " [99, 269, 371, 1115, 582, 52, 7, 2, 372],\n",
       " [99, 269, 371, 1115, 582, 52, 7, 2, 372, 10],\n",
       " [99, 269, 371, 1115, 582, 52, 7, 2, 372, 10, 1116],\n",
       " [100, 3]]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습데이터 준비\n",
    "sequences = list()\n",
    "\n",
    "for line in text: # 1,214 개의 샘플에 대해서 샘플을 1개씩 가져온다.\n",
    "    encoded = t.texts_to_sequences([line])[0] # 각 샘플에 대한 정수 인코딩\n",
    "    for i in range(1, len(encoded)):\n",
    "        sequence = encoded[:i+1]\n",
    "        sequences.append(sequence)\n",
    "\n",
    "sequences[:11] # 11개의 샘플 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "빈도수 상위 582번 단어 : offer\n"
     ]
    }
   ],
   "source": [
    "# 인덱스 숫자와 단어 확인\n",
    "index_to_word={}\n",
    "for key, value in t.word_index.items(): # 인덱스를 단어로 바꾸기 위해 index_to_word를 생성\n",
    "    index_to_word[value] = key\n",
    "\n",
    "print('빈도수 상위 582번 단어 : {}'.format(index_to_word[582]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플의 최대 길이 : 24\n"
     ]
    }
   ],
   "source": [
    "# 샘플 길이 padding\n",
    "max_len=max(len(l) for l in sequences)\n",
    "print('샘플의 최대 길이 : {}'.format(max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0   99  269]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0   99  269  371]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0   99  269  371 1115]]\n"
     ]
    }
   ],
   "source": [
    "sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')\n",
    "print(sequences[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/label 분리\n",
    "sequences = np.array(sequences)\n",
    "X = sequences[:,:-1]\n",
    "y = sequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label one-hot-encoding\n",
    "y = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\KMS\\Anaconda3\\envs\\data37\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 7803 samples\n",
      "Epoch 1/200\n",
      "7803/7803 - 6s - loss: 7.6390 - acc: 0.0293\n",
      "Epoch 2/200\n",
      "7803/7803 - 6s - loss: 7.1206 - acc: 0.0306\n",
      "Epoch 3/200\n",
      "7803/7803 - 6s - loss: 6.9851 - acc: 0.0347\n",
      "Epoch 4/200\n",
      "7803/7803 - 6s - loss: 6.8553 - acc: 0.0417\n",
      "Epoch 5/200\n",
      "7803/7803 - 6s - loss: 6.7025 - acc: 0.0451\n",
      "Epoch 6/200\n",
      "7803/7803 - 6s - loss: 6.5293 - acc: 0.0482\n",
      "Epoch 7/200\n",
      "7803/7803 - 6s - loss: 6.3414 - acc: 0.0541\n",
      "Epoch 8/200\n",
      "7803/7803 - 6s - loss: 6.1462 - acc: 0.0555\n",
      "Epoch 9/200\n",
      "7803/7803 - 6s - loss: 5.9540 - acc: 0.0616\n",
      "Epoch 10/200\n",
      "7803/7803 - 6s - loss: 5.7675 - acc: 0.0673\n",
      "Epoch 11/200\n",
      "7803/7803 - 6s - loss: 5.5901 - acc: 0.0698\n",
      "Epoch 12/200\n",
      "7803/7803 - 6s - loss: 5.4226 - acc: 0.0754\n",
      "Epoch 13/200\n",
      "7803/7803 - 6s - loss: 5.2588 - acc: 0.0841\n",
      "Epoch 14/200\n",
      "7803/7803 - 6s - loss: 5.1049 - acc: 0.0887\n",
      "Epoch 15/200\n",
      "7803/7803 - 6s - loss: 4.9560 - acc: 0.0953\n",
      "Epoch 16/200\n",
      "7803/7803 - 6s - loss: 4.8171 - acc: 0.1060\n",
      "Epoch 17/200\n",
      "7803/7803 - 6s - loss: 4.6786 - acc: 0.1178\n",
      "Epoch 18/200\n",
      "7803/7803 - 6s - loss: 4.5469 - acc: 0.1310\n",
      "Epoch 19/200\n",
      "7803/7803 - 6s - loss: 4.4191 - acc: 0.1480\n",
      "Epoch 20/200\n",
      "7803/7803 - 6s - loss: 4.2947 - acc: 0.1663\n",
      "Epoch 21/200\n",
      "7803/7803 - 7s - loss: 4.1779 - acc: 0.1820\n",
      "Epoch 22/200\n",
      "7803/7803 - 6s - loss: 4.0579 - acc: 0.2052\n",
      "Epoch 23/200\n",
      "7803/7803 - 6s - loss: 3.9469 - acc: 0.2188\n",
      "Epoch 24/200\n",
      "7803/7803 - 6s - loss: 3.8346 - acc: 0.2411\n",
      "Epoch 25/200\n",
      "7803/7803 - 7s - loss: 3.7327 - acc: 0.2594\n",
      "Epoch 26/200\n",
      "7803/7803 - 6s - loss: 3.6238 - acc: 0.2750\n",
      "Epoch 27/200\n",
      "7803/7803 - 6s - loss: 3.5216 - acc: 0.2969\n",
      "Epoch 28/200\n",
      "7803/7803 - 6s - loss: 3.4262 - acc: 0.3095\n",
      "Epoch 29/200\n",
      "7803/7803 - 6s - loss: 3.3318 - acc: 0.3253\n",
      "Epoch 30/200\n",
      "7803/7803 - 6s - loss: 3.2421 - acc: 0.3437\n",
      "Epoch 31/200\n",
      "7803/7803 - 7s - loss: 3.1534 - acc: 0.3601\n",
      "Epoch 32/200\n",
      "7803/7803 - 6s - loss: 3.0676 - acc: 0.3749\n",
      "Epoch 33/200\n",
      "7803/7803 - 6s - loss: 2.9839 - acc: 0.3907\n",
      "Epoch 34/200\n",
      "7803/7803 - 6s - loss: 2.9098 - acc: 0.4018\n",
      "Epoch 35/200\n",
      "7803/7803 - 6s - loss: 2.8307 - acc: 0.4219\n",
      "Epoch 36/200\n",
      "7803/7803 - 6s - loss: 2.7576 - acc: 0.4342\n",
      "Epoch 37/200\n",
      "7803/7803 - 6s - loss: 2.6852 - acc: 0.4459\n",
      "Epoch 38/200\n",
      "7803/7803 - 6s - loss: 2.6176 - acc: 0.4603\n",
      "Epoch 39/200\n",
      "7803/7803 - 6s - loss: 2.5503 - acc: 0.4797\n",
      "Epoch 40/200\n",
      "7803/7803 - 6s - loss: 2.4870 - acc: 0.4887\n",
      "Epoch 41/200\n",
      "7803/7803 - 6s - loss: 2.4245 - acc: 0.5028\n",
      "Epoch 42/200\n",
      "7803/7803 - 6s - loss: 2.3657 - acc: 0.5135\n",
      "Epoch 43/200\n",
      "7803/7803 - 6s - loss: 2.3030 - acc: 0.5249\n",
      "Epoch 44/200\n",
      "7803/7803 - 6s - loss: 2.2471 - acc: 0.5331\n",
      "Epoch 45/200\n",
      "7803/7803 - 6s - loss: 2.1914 - acc: 0.5467\n",
      "Epoch 46/200\n",
      "7803/7803 - 6s - loss: 2.1392 - acc: 0.5594\n",
      "Epoch 47/200\n",
      "7803/7803 - 6s - loss: 2.0864 - acc: 0.5713\n",
      "Epoch 48/200\n",
      "7803/7803 - 6s - loss: 2.0397 - acc: 0.5771\n",
      "Epoch 49/200\n",
      "7803/7803 - 6s - loss: 1.9885 - acc: 0.5875\n",
      "Epoch 50/200\n",
      "7803/7803 - 6s - loss: 1.9378 - acc: 0.6005\n",
      "Epoch 51/200\n",
      "7803/7803 - 6s - loss: 1.8920 - acc: 0.6081\n",
      "Epoch 52/200\n",
      "7803/7803 - 6s - loss: 1.8467 - acc: 0.6189\n",
      "Epoch 53/200\n",
      "7803/7803 - 6s - loss: 1.8050 - acc: 0.6278\n",
      "Epoch 54/200\n",
      "7803/7803 - 6s - loss: 1.7592 - acc: 0.6398\n",
      "Epoch 55/200\n",
      "7803/7803 - 6s - loss: 1.7182 - acc: 0.6441\n",
      "Epoch 56/200\n",
      "7803/7803 - 6s - loss: 1.6755 - acc: 0.6556\n",
      "Epoch 57/200\n",
      "7803/7803 - 6s - loss: 1.6367 - acc: 0.6623\n",
      "Epoch 58/200\n",
      "7803/7803 - 6s - loss: 1.5999 - acc: 0.6733\n",
      "Epoch 59/200\n",
      "7803/7803 - 6s - loss: 1.5596 - acc: 0.6787\n",
      "Epoch 60/200\n",
      "7803/7803 - 6s - loss: 1.5241 - acc: 0.6881\n",
      "Epoch 61/200\n",
      "7803/7803 - 7s - loss: 1.4905 - acc: 0.6954\n",
      "Epoch 62/200\n",
      "7803/7803 - 7s - loss: 1.4541 - acc: 0.7023\n",
      "Epoch 63/200\n",
      "7803/7803 - 7s - loss: 1.4224 - acc: 0.7078\n",
      "Epoch 64/200\n",
      "7803/7803 - 6s - loss: 1.3884 - acc: 0.7173\n",
      "Epoch 65/200\n",
      "7803/7803 - 6s - loss: 1.3534 - acc: 0.7259\n",
      "Epoch 66/200\n",
      "7803/7803 - 6s - loss: 1.3222 - acc: 0.7342\n",
      "Epoch 67/200\n",
      "7803/7803 - 7s - loss: 1.2923 - acc: 0.7366\n",
      "Epoch 68/200\n",
      "7803/7803 - 6s - loss: 1.2657 - acc: 0.7457\n",
      "Epoch 69/200\n",
      "7803/7803 - 6s - loss: 1.2347 - acc: 0.7504\n",
      "Epoch 70/200\n",
      "7803/7803 - 6s - loss: 1.2053 - acc: 0.7554\n",
      "Epoch 71/200\n",
      "7803/7803 - 6s - loss: 1.1751 - acc: 0.7652\n",
      "Epoch 72/200\n",
      "7803/7803 - 6s - loss: 1.1510 - acc: 0.7689\n",
      "Epoch 73/200\n",
      "7803/7803 - 6s - loss: 1.1220 - acc: 0.7755\n",
      "Epoch 74/200\n",
      "7803/7803 - 6s - loss: 1.0952 - acc: 0.7820\n",
      "Epoch 75/200\n",
      "7803/7803 - 6s - loss: 1.0703 - acc: 0.7905\n",
      "Epoch 76/200\n",
      "7803/7803 - 6s - loss: 1.0491 - acc: 0.7928\n",
      "Epoch 77/200\n",
      "7803/7803 - 6s - loss: 1.0256 - acc: 0.7979\n",
      "Epoch 78/200\n",
      "7803/7803 - 6s - loss: 1.0018 - acc: 0.7987\n",
      "Epoch 79/200\n",
      "7803/7803 - 7s - loss: 0.9758 - acc: 0.8064\n",
      "Epoch 80/200\n",
      "7803/7803 - 7s - loss: 0.9550 - acc: 0.8115\n",
      "Epoch 81/200\n",
      "7803/7803 - 6s - loss: 0.9313 - acc: 0.8166\n",
      "Epoch 82/200\n",
      "7803/7803 - 7s - loss: 0.9129 - acc: 0.8206\n",
      "Epoch 83/200\n",
      "7803/7803 - 6s - loss: 0.8927 - acc: 0.8231\n",
      "Epoch 84/200\n",
      "7803/7803 - 7s - loss: 0.8746 - acc: 0.8240\n",
      "Epoch 85/200\n",
      "7803/7803 - 6s - loss: 0.8540 - acc: 0.8303\n",
      "Epoch 86/200\n",
      "7803/7803 - 6s - loss: 0.8338 - acc: 0.8325\n",
      "Epoch 87/200\n",
      "7803/7803 - 6s - loss: 0.8125 - acc: 0.8404\n",
      "Epoch 88/200\n",
      "7803/7803 - 7s - loss: 0.7978 - acc: 0.8415\n",
      "Epoch 89/200\n",
      "7803/7803 - 7s - loss: 0.7801 - acc: 0.8438\n",
      "Epoch 90/200\n",
      "7803/7803 - 7s - loss: 0.7644 - acc: 0.8495\n",
      "Epoch 91/200\n",
      "7803/7803 - 6s - loss: 0.7476 - acc: 0.8512\n",
      "Epoch 92/200\n",
      "7803/7803 - 6s - loss: 0.7291 - acc: 0.8542\n",
      "Epoch 93/200\n",
      "7803/7803 - 6s - loss: 0.7159 - acc: 0.8580\n",
      "Epoch 94/200\n",
      "7803/7803 - 6s - loss: 0.6998 - acc: 0.8625\n",
      "Epoch 95/200\n",
      "7803/7803 - 6s - loss: 0.6914 - acc: 0.8629\n",
      "Epoch 96/200\n",
      "7803/7803 - 7s - loss: 0.6766 - acc: 0.8654\n",
      "Epoch 97/200\n",
      "7803/7803 - 7s - loss: 0.6561 - acc: 0.8722\n",
      "Epoch 98/200\n",
      "7803/7803 - 7s - loss: 0.6428 - acc: 0.8717\n",
      "Epoch 99/200\n",
      "7803/7803 - 7s - loss: 0.6283 - acc: 0.8757\n",
      "Epoch 100/200\n",
      "7803/7803 - 7s - loss: 0.6163 - acc: 0.8771\n",
      "Epoch 101/200\n",
      "7803/7803 - 7s - loss: 0.6035 - acc: 0.8803\n",
      "Epoch 102/200\n",
      "7803/7803 - 7s - loss: 0.5941 - acc: 0.8797\n",
      "Epoch 103/200\n",
      "7803/7803 - 7s - loss: 0.5821 - acc: 0.8815\n",
      "Epoch 104/200\n",
      "7803/7803 - 7s - loss: 0.5724 - acc: 0.8856\n",
      "Epoch 105/200\n",
      "7803/7803 - 6s - loss: 0.5620 - acc: 0.8850\n",
      "Epoch 106/200\n",
      "7803/7803 - 7s - loss: 0.5515 - acc: 0.8890\n",
      "Epoch 107/200\n",
      "7803/7803 - 7s - loss: 0.5471 - acc: 0.8920\n",
      "Epoch 108/200\n",
      "7803/7803 - 7s - loss: 0.5283 - acc: 0.8927\n",
      "Epoch 109/200\n",
      "7803/7803 - 7s - loss: 0.5248 - acc: 0.8953\n",
      "Epoch 110/200\n",
      "7803/7803 - 7s - loss: 0.5123 - acc: 0.8972\n",
      "Epoch 111/200\n",
      "7803/7803 - 7s - loss: 0.5018 - acc: 0.8993\n",
      "Epoch 112/200\n",
      "7803/7803 - 7s - loss: 0.4904 - acc: 0.8984\n",
      "Epoch 113/200\n",
      "7803/7803 - 7s - loss: 0.4832 - acc: 0.8977\n",
      "Epoch 114/200\n",
      "7803/7803 - 7s - loss: 0.4733 - acc: 0.9021\n",
      "Epoch 115/200\n",
      "7803/7803 - 7s - loss: 0.4674 - acc: 0.9030\n",
      "Epoch 116/200\n",
      "7803/7803 - 7s - loss: 0.4659 - acc: 0.9034\n",
      "Epoch 117/200\n",
      "7803/7803 - 7s - loss: 0.4572 - acc: 0.9041\n",
      "Epoch 118/200\n",
      "7803/7803 - 7s - loss: 0.4433 - acc: 0.9040\n",
      "Epoch 119/200\n",
      "7803/7803 - 7s - loss: 0.4468 - acc: 0.9041\n",
      "Epoch 120/200\n",
      "7803/7803 - 7s - loss: 0.4350 - acc: 0.9076\n",
      "Epoch 121/200\n",
      "7803/7803 - 7s - loss: 0.4217 - acc: 0.9097\n",
      "Epoch 122/200\n",
      "7803/7803 - 6s - loss: 0.4155 - acc: 0.9088\n",
      "Epoch 123/200\n",
      "7803/7803 - 7s - loss: 0.4119 - acc: 0.9100\n",
      "Epoch 124/200\n",
      "7803/7803 - 7s - loss: 0.4056 - acc: 0.9091\n",
      "Epoch 125/200\n",
      "7803/7803 - 6s - loss: 0.4075 - acc: 0.9089\n",
      "Epoch 126/200\n",
      "7803/7803 - 6s - loss: 0.4025 - acc: 0.9097\n",
      "Epoch 127/200\n",
      "7803/7803 - 6s - loss: 0.3994 - acc: 0.9116\n",
      "Epoch 128/200\n",
      "7803/7803 - 6s - loss: 0.3875 - acc: 0.9117\n",
      "Epoch 129/200\n",
      "7803/7803 - 6s - loss: 0.3800 - acc: 0.9138\n",
      "Epoch 130/200\n",
      "7803/7803 - 6s - loss: 0.3758 - acc: 0.9127\n",
      "Epoch 131/200\n",
      "7803/7803 - 6s - loss: 0.3706 - acc: 0.9129\n",
      "Epoch 132/200\n",
      "7803/7803 - 6s - loss: 0.3651 - acc: 0.9138\n",
      "Epoch 133/200\n",
      "7803/7803 - 6s - loss: 0.3605 - acc: 0.9152\n",
      "Epoch 134/200\n",
      "7803/7803 - 6s - loss: 0.3610 - acc: 0.9143\n",
      "Epoch 135/200\n",
      "7803/7803 - 6s - loss: 0.3554 - acc: 0.9162\n",
      "Epoch 136/200\n",
      "7803/7803 - 6s - loss: 0.3521 - acc: 0.9157\n",
      "Epoch 137/200\n",
      "7803/7803 - 6s - loss: 0.3497 - acc: 0.9145\n",
      "Epoch 138/200\n",
      "7803/7803 - 6s - loss: 0.3507 - acc: 0.9129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139/200\n",
      "7803/7803 - 7s - loss: 0.3905 - acc: 0.9070\n",
      "Epoch 140/200\n",
      "7803/7803 - 6s - loss: 0.3693 - acc: 0.9111\n",
      "Epoch 141/200\n",
      "7803/7803 - 6s - loss: 0.3448 - acc: 0.9139\n",
      "Epoch 142/200\n",
      "7803/7803 - 6s - loss: 0.3337 - acc: 0.9154\n",
      "Epoch 143/200\n",
      "7803/7803 - 7s - loss: 0.3250 - acc: 0.9152\n",
      "Epoch 144/200\n",
      "7803/7803 - 6s - loss: 0.3228 - acc: 0.9154\n",
      "Epoch 145/200\n",
      "7803/7803 - 6s - loss: 0.3207 - acc: 0.9157\n",
      "Epoch 146/200\n",
      "7803/7803 - 6s - loss: 0.3244 - acc: 0.9167\n",
      "Epoch 147/200\n",
      "7803/7803 - 6s - loss: 0.3199 - acc: 0.9161\n",
      "Epoch 148/200\n",
      "7803/7803 - 6s - loss: 0.3167 - acc: 0.9148\n",
      "Epoch 149/200\n",
      "7803/7803 - 6s - loss: 0.3141 - acc: 0.9157\n",
      "Epoch 150/200\n",
      "7803/7803 - 7s - loss: 0.3127 - acc: 0.9166\n",
      "Epoch 151/200\n",
      "7803/7803 - 7s - loss: 0.3108 - acc: 0.9159\n",
      "Epoch 152/200\n",
      "7803/7803 - 6s - loss: 0.3103 - acc: 0.9171\n",
      "Epoch 153/200\n",
      "7803/7803 - 7s - loss: 0.3148 - acc: 0.9177\n",
      "Epoch 154/200\n",
      "7803/7803 - 7s - loss: 0.3099 - acc: 0.9164\n",
      "Epoch 155/200\n",
      "7803/7803 - 7s - loss: 0.3116 - acc: 0.9153\n",
      "Epoch 156/200\n",
      "7803/7803 - 6s - loss: 0.3059 - acc: 0.9162\n",
      "Epoch 157/200\n",
      "7803/7803 - 6s - loss: 0.2999 - acc: 0.9159\n",
      "Epoch 158/200\n",
      "7803/7803 - 6s - loss: 0.2993 - acc: 0.9162\n",
      "Epoch 159/200\n",
      "7803/7803 - 6s - loss: 0.2960 - acc: 0.9167\n",
      "Epoch 160/200\n",
      "7803/7803 - 6s - loss: 0.2979 - acc: 0.9155\n",
      "Epoch 161/200\n",
      "7803/7803 - 6s - loss: 0.2961 - acc: 0.9164\n",
      "Epoch 162/200\n",
      "7803/7803 - 6s - loss: 0.3020 - acc: 0.9149\n",
      "Epoch 163/200\n",
      "7803/7803 - 6s - loss: 0.2957 - acc: 0.9150\n",
      "Epoch 164/200\n",
      "7803/7803 - 6s - loss: 0.2931 - acc: 0.9143\n",
      "Epoch 165/200\n",
      "7803/7803 - 6s - loss: 0.2941 - acc: 0.9164\n",
      "Epoch 166/200\n",
      "7803/7803 - 6s - loss: 0.3163 - acc: 0.9102\n",
      "Epoch 167/200\n",
      "7803/7803 - 6s - loss: 0.3313 - acc: 0.9107\n",
      "Epoch 168/200\n",
      "7803/7803 - 6s - loss: 0.2970 - acc: 0.9168\n",
      "Epoch 169/200\n",
      "7803/7803 - 6s - loss: 0.2876 - acc: 0.9154\n",
      "Epoch 170/200\n",
      "7803/7803 - 6s - loss: 0.2826 - acc: 0.9161\n",
      "Epoch 171/200\n",
      "7803/7803 - 7s - loss: 0.2811 - acc: 0.9190\n",
      "Epoch 172/200\n",
      "7803/7803 - 7s - loss: 0.2818 - acc: 0.9155\n",
      "Epoch 173/200\n",
      "7803/7803 - 7s - loss: 0.2815 - acc: 0.9168\n",
      "Epoch 174/200\n",
      "7803/7803 - 7s - loss: 0.2796 - acc: 0.9155\n",
      "Epoch 175/200\n",
      "7803/7803 - 6s - loss: 0.2801 - acc: 0.9168\n",
      "Epoch 176/200\n",
      "7803/7803 - 6s - loss: 0.2820 - acc: 0.9161\n",
      "Epoch 177/200\n",
      "7803/7803 - 7s - loss: 0.2806 - acc: 0.9161\n",
      "Epoch 178/200\n",
      "7803/7803 - 7s - loss: 0.2777 - acc: 0.9170\n",
      "Epoch 179/200\n",
      "7803/7803 - 7s - loss: 0.2811 - acc: 0.9150\n",
      "Epoch 180/200\n",
      "7803/7803 - 6s - loss: 0.2766 - acc: 0.9170\n",
      "Epoch 181/200\n",
      "7803/7803 - 6s - loss: 0.2824 - acc: 0.9155\n",
      "Epoch 182/200\n",
      "7803/7803 - 6s - loss: 0.3197 - acc: 0.9112\n",
      "Epoch 183/200\n",
      "7803/7803 - 6s - loss: 0.3134 - acc: 0.9099\n",
      "Epoch 184/200\n",
      "7803/7803 - 6s - loss: 0.2831 - acc: 0.9159\n",
      "Epoch 185/200\n",
      "7803/7803 - 7s - loss: 0.2755 - acc: 0.9180\n",
      "Epoch 186/200\n",
      "7803/7803 - 6s - loss: 0.2722 - acc: 0.9158\n",
      "Epoch 187/200\n",
      "7803/7803 - 6s - loss: 0.2722 - acc: 0.9161\n",
      "Epoch 188/200\n",
      "7803/7803 - 6s - loss: 0.2714 - acc: 0.9167\n",
      "Epoch 189/200\n",
      "7803/7803 - 6s - loss: 0.2715 - acc: 0.9146\n",
      "Epoch 190/200\n",
      "7803/7803 - 6s - loss: 0.2700 - acc: 0.9166\n",
      "Epoch 191/200\n",
      "7803/7803 - 6s - loss: 0.2715 - acc: 0.9168\n",
      "Epoch 192/200\n",
      "7803/7803 - 6s - loss: 0.2718 - acc: 0.9153\n",
      "Epoch 193/200\n",
      "7803/7803 - 6s - loss: 0.2711 - acc: 0.9155\n",
      "Epoch 194/200\n",
      "7803/7803 - 7s - loss: 0.2760 - acc: 0.9155\n",
      "Epoch 195/200\n",
      "7803/7803 - 6s - loss: 0.2788 - acc: 0.9146\n",
      "Epoch 196/200\n",
      "7803/7803 - 6s - loss: 0.2790 - acc: 0.9152\n",
      "Epoch 197/200\n",
      "7803/7803 - 6s - loss: 0.2800 - acc: 0.9158\n",
      "Epoch 198/200\n",
      "7803/7803 - 6s - loss: 0.2892 - acc: 0.9154\n",
      "Epoch 199/200\n",
      "7803/7803 - 6s - loss: 0.2744 - acc: 0.9152\n",
      "Epoch 200/200\n",
      "7803/7803 - 7s - loss: 0.2702 - acc: 0.9162\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f374235cc8>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#모델 생성\n",
    "# 6s/epoch -> \n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=max_len-1))\n",
    "# y데이터를 분리하였으므로 이제 X데이터의 길이는 기존 데이터의 길이 - 1\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=200, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_X = X[0].reshape(1,23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.8096646e-11, 1.4314705e-06, 2.1641354e-06, ..., 3.9318615e-09,\n",
       "        1.8927975e-07, 8.9086463e-09]], dtype=float32)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(reshape_X,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([269], dtype=int64)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(reshape_X,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임의의 단어 이용 다음 단어 예측\n",
    "def sentence_generation(model, t, current_word, n): # 모델, 토크나이저, 현재 단어, 반복할 횟수\n",
    "    init_word = current_word # 처음 들어온 단어도 마지막에 같이 출력하기위해 저장\n",
    "    sentence = ''\n",
    "    for _ in range(n): # n번 반복\n",
    "        encoded = t.texts_to_sequences([current_word])[0] # 현재 단어에 대한 정수 인코딩\n",
    "        encoded = pad_sequences([encoded], maxlen=23, padding='pre') # 데이터에 대한 패딩\n",
    "        print(encoded.shape)\n",
    "        result = model.predict_classes(encoded, verbose=0)\n",
    "    # 입력한 X(현재 단어)에 대해서 y를 예측하고 y(예측한 단어)를 result에 저장.\n",
    "        for word, index in t.word_index.items(): \n",
    "            if index == result: # 만약 예측한 단어와 인덱스와 동일한 단어가 있다면\n",
    "                break # 해당 단어가 예측 단어이므로 break\n",
    "        current_word = current_word + ' '  + word # 현재 단어 + ' ' + 예측 단어를 현재 단어로 변경\n",
    "        sentence = sentence + ' ' + word # 예측 단어를 문장에 저장\n",
    "    # for문이므로 이 행동을 다시 반복\n",
    "    sentence = init_word + sentence\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 23)\n",
      "i disapprove\n"
     ]
    }
   ],
   "source": [
    "print(sentence_generation(model, t, 'i', 1))\n",
    "# 임의의 단어 'i'에 대해서 10개의 단어를 추가 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how to make facebook more accountable hes beat trump but a\n"
     ]
    }
   ],
   "source": [
    "print(sentence_generation(model, t, 'how', 10))\n",
    "# 임의의 단어 'how'에 대해서 10개의 단어를 추가 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
