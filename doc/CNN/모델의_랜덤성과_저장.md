## 들어가기 전에 - 딥러닝의 랜덤성
딥러닝 모델의 학습 과정 중에는 굉장히 많은 부분에서 random성이 개입한다.

1. 데이터 Split  
train/validation을 분할하는 과정에서 이미 랜덤성이 개입한다
2. Initializer  
모델의 초기상태를 지정하는 initializer 또한 랜덤성이 개입한다.
3. Dropout  
대표적인것이 Dropout으로, Dropout으로 인해 제외되는 layer는 랜덤으로 결정된다.

위와 같은 사항으로 인해 동일 모델을 동일 parameter로 돌린다고 해도 **완전히 동일한 결과를 보장하지 못한다.**  
따라서, 최대한 유사한 결과를 재현하기 위해서는 **해당 모델의 학습당시 상황을 최대한 기록하고 snapshot으로 저장하여, 그대로 불러와 재현할 수 있게 하는 것이 중요**하다.  
저장해야 할 요소들은 크게 세가지 정도로 정리할 수 있을 것 같다.

## 시스템 환경 변수들
생각나는 시스템 변수들은 아래와 같다.

>테스트, 트레인 데이터가 저장된 경로 등 시스템 변수들

위의 변수들은 성능에는 영향은 없지만, 모델의 리팩토링 관점에서 저장해두면 좋을 것 같다.

#### 시스템 환경변수 저장
공통 프로젝트의 config.py 와 같이 저장하면 된다.
## 모델 내부 구조의 저장
모델 내부 구조란, 모델 내부에서 퍼셉트론들이 어떤 형태로 구성되었는가 정보를 말한다

> Layer의 깊이, Layer별 node의 개수, Layer의 activation function& initialize function, Dropout 레이어 위치, Optimization fuinction과 learning rate


## 모델 학습 결과의 저장
모델 학습 결과란, 학습을 위해 지정된 parameter들과 이로부터 학습 결과 생성되는 parameter를 의미한다.  
> 학습 parameter : train/validaion sample ratio, batch size, epoch, class별 weight

> 학습 결과 parameter : 모델의 weight, 학습 metric의 값과 그래프

## 코드
모델 저장 예제 코드는 아래 링크에 있다
[jupyter notebook](doc/Notebook/save-model-history.ipynb)