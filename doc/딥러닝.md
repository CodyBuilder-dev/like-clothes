# 이미지 캡셔닝 성능 향상 기법

마이너하게 수정해야 할것!

이거 모두 따로 설정할 수 있도록 바꿔주자. 

너무나도 많다. 

## 토큰화

토큰화 방식이 엄청 많다... 바꿀 수 있지 않을까?

방식이 많지만, 영어 토큰화 방식에 대해서는 지금 방식이 괜찮은듯





## 드롭아웃

rnn같은 경우에는 과적합되기 쉬움. 그래서 드랍아웃도 많이 넣는 경우가 많음.



## LSTM

RNN

핍홉 연결(peephole connection) LSTM의 변종 

GRU 사용 : 학습속도가 빠르다. LSTM이랑 비슷한 성능, 일부 논문에서는 GRU보다 LSTM이 더 뛰어난 성능을 보인다. 

model.add(GRU(hidden_size, input_shape=(timesteps, input_dim)))model.add(GRU(hidden_size, input_shape=(timesteps, input_dim)))

GRU 이중사용

LSTM 사용

LSTM 이중사용

양방향 LSTM

트랜스포머  -> 엄청 빠르다고 함. 연구가 진행 중



## 어텐션

신경망 성능을 위한 메커니즘이자, 이제는 AI 분야의 대세 모듈로서 사용됨.



디코더에서 출력 단어를 예측하는 매 시점마다, 인코더에서의 전체 입력 문장을 다시 한 번 참고한다. 단, 전체 입력 문장을 전부 다 동일한 비율로 참고한 것 ㄴㄴ

해당 시점에 예측해야할 단어와 연관이 있는 입력 단어 부분을 집중(attention)해서 봄



케라스 기본이 롱 attentio mechanism임.

![image-20200410133232500](C:\Users\Lee\AppData\Roaming\Typora\typora-user-images\image-20200410133232500.png)



![image-20200410132427196](C:\Users\Lee\AppData\Roaming\Typora\typora-user-images\image-20200410132427196.png)





## AI 서버에서 모델 예측 시도

잘 됨.

입력 데이터가 올 때마다 직렬화를 통해서 그 때 그때 모델을 업데이트함.

그러나 동시 사용자가 많아지면 피클이 깨질 수도있다고 하여 중단.

